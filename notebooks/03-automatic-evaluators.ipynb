{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 · Automated Evaluators for the Email Summarizer\n",
    "\n",
    "This notebook sits in the **Measure** phase of the workshop. Earlier notebooks surfaced failure modes and produced labeled traces. Here we explain why automated evaluators matter, compare reference-free and reference-based approaches for the email summarizer, and show how programmatic checks and LLM-as-judge workflows accelerate iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Automated Evaluators?\n",
    "- Manual re-labeling after every prompt tweak is slow and inconsistent. Automated evaluators let us re-measure a new run of summaries in minutes rather than hours.\n",
    "- The Analyze → Measure → Improve loop depends on trustworthy metrics. Code or LLM judges give us repeatable estimates of how often the summarizer still fails.\n",
    "- Automation is especially valuable for the Enron email summarizer because many prompts need small wording changes; we do not want to hand-label the same trace set after every tweak.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca45ef9",
   "metadata": {},
   "source": [
    "## Reference-Free vs Reference-Based Metrics\n",
    "Reference-free metrics inspect the model output directly, while reference-based metrics compare it to a trusted target (for example, human-written summary bullets). In practice we stack both to cover different failure surfaces.\n",
    "\n",
    "| Metric Type        | Summarizer Example                                        | What It Checks                                | Strengths                                   | Considerations |\n",
    "|--------------------|-----------------------------------------------------------|------------------------------------------------|---------------------------------------------|----------------|\n",
    "| Reference-Free     | Flag summaries that sound too casual for executive readers | Tone, structure, presence of a CTA             | Cheap, deterministic, easy to debug         | Misses nuanced factual errors |\n",
    "| Reference-Based    | Compare generated bullets to analyst-written gold bullets  | Coverage of key decisions / action items       | Precise fidelity check                       | Needs curated references per email |\n",
    "| Hybrid             | Require both tone check and coverage check                | Tone + correctness in one report               | Broader failure coverage                     | Higher maintenance across metrics |\n",
    "\n",
    "The rest of this notebook uses synthetic data shaped like our email summarizer traces to illustrate each style of evaluator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bf97b",
   "metadata": {},
   "source": [
    "## Programmatic Evaluators for Informal Tone\n",
    "When the failure definition is objective (e.g., \"+/-\" on informal phrases), a programmatic evaluator is fast and reliable. Below we flag summaries that include slang we have agreed is unacceptable for client-facing communications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INFORMAL_KEYWORDS = {\n",
    "    \"hey team\",\n",
    "    \"super pumped\",\n",
    "    \"lol\",\n",
    "    \"cheers\",\n",
    "    \"you guys\",\n",
    "    \"gonna\",\n",
    "}\n",
    "\n",
    "sample_summaries = pd.DataFrame([\n",
    "    {\n",
    "        \"summary_id\": \"S-001\",\n",
    "        \"summary\": \"Team — Here's the recap: Decisions were documented, and Finance will review numbers tomorrow. No informal tone here.\",\n",
    "        \"label\": \"Human Pass\",\n",
    "    },\n",
    "    {\n",
    "        \"summary_id\": \"S-002\",\n",
    "        \"summary\": \"Hey team! Super pumped about the vendor shortlist. You guys should ping me if anything feels off.\",\n",
    "        \"label\": \"Human Fail (too casual)\",\n",
    "    },\n",
    "    {\n",
    "        \"summary_id\": \"S-003\",\n",
    "        \"summary\": \"The group confirmed the migration timeline. Cheers, and let's lock in the training invites.\",\n",
    "        \"label\": \"Human Fail (casual sign-off)\",\n",
    "    },\n",
    "    {\n",
    "        \"summary_id\": \"S-004\",\n",
    "        \"summary\": \"Team — All action items remain with Ops; no tone violations were spotted.\",\n",
    "        \"label\": \"Human Pass\",\n",
    "    },\n",
    "])\n",
    "\n",
    "\n",
    "def detect_informal_tone(text: str):\n",
    "    lowered = text.lower()\n",
    "    hits = [kw for kw in INFORMAL_KEYWORDS if kw in lowered]\n",
    "    return hits\n",
    "\n",
    "\n",
    "sample_summaries[\"informal_hits\"] = sample_summaries[\"summary\"].apply(detect_informal_tone)\n",
    "sample_summaries[\"flagged_informal\"] = sample_summaries[\"informal_hits\"].apply(bool)\n",
    "sample_summaries[[\"summary_id\", \"flagged_informal\", \"informal_hits\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c6c8d",
   "metadata": {},
   "source": [
    "The heuristic catches rows S-002 and S-003 because they contain agreed-upon informal phrases. Programmatic evaluators like this are ideal for specification failures we can encode as deterministic rules.\n",
    "\n",
    "For nuanced behaviors such as “Did the summary capture every decision from the thread without hallucinating?” we lean on an LLM-as-judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72e885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "405f251b",
   "metadata": {},
   "source": [
    "### Dataset Setup and Stratified Splits\n",
    "We'll start from the merged human-labeled set `data/llm-judge-sample-full.json` and carve out 15%/40%/45% train/validation/test splits while preserving the pass/fail balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028489db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_PATH = Path(\"../data/llm-judge-sample-full.json\")\n",
    "\n",
    "judge_df = pd.read_json(DATA_PATH)\n",
    "judge_df[\"human_judgement\"] = judge_df[\"human_judgement\"].str.upper()\n",
    "\n",
    "label_counts = (\n",
    "    judge_df[\"human_judgement\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"label\")\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "label_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FRACTIONS = (0.15, 0.40, 0.45)\n",
    "\n",
    "splits = stratified_split_sklearn(\n",
    "    judge_df,\n",
    "    label_col=\"human_judgement\",\n",
    "    fractions=SPLIT_FRACTIONS,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "split_summary = (\n",
    "    pd.concat(\n",
    "        {\n",
    "            split_name: df[\"human_judgement\"].value_counts()\n",
    "            for split_name, df in splits.items()\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .rename_axis(\"label\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "split_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de479c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FRACTIONS = (0.15, 0.40, 0.45)\n",
    "\n",
    "# --- Step 1: Split using sklearn-based stratified split ---\n",
    "splits = stratified_split_sklearn(\n",
    "    judge_df,\n",
    "    label_col=\"human_judgement\",\n",
    "    fractions=SPLIT_FRACTIONS,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# --- Step 2: Check split summary (unchanged) ---\n",
    "split_summary = (\n",
    "    pd.concat(\n",
    "        {\n",
    "            split_name: df[\"human_judgement\"].value_counts()\n",
    "            for split_name, df in splits.items()\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .rename_axis(\"label\")\n",
    "    .sort_index()\n",
    ")\n",
    "print(split_summary)\n",
    "\n",
    "# --- Step 3: Write splits to JSON files (unchanged) ---\n",
    "from pathlib import Path\n",
    "\n",
    "for split_name, df in splits.items():\n",
    "    output_path = Path(f\"../data/llm-judge-split-{split_name}.json\")\n",
    "    output_path.write_text(df.to_json(orient=\"records\", indent=2))\n",
    "    print(f\"Wrote {len(df)} rows to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b21e1",
   "metadata": {},
   "source": [
    "### Few-shot Prompt Builder\n",
    "Select a handful of pass/fail examples from the train split so the judge can learn what logically coherent summaries look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from textwrap import dedent\n",
    "import random\n",
    "\n",
    "\n",
    "def sample_few_shot_examples(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str,\n",
    "    per_label: dict,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> pd.DataFrame:\n",
    "    rng = random.Random(seed)\n",
    "    selections = []\n",
    "    for label, quota in per_label.items():\n",
    "        pool = df[df[label_col] == label]\n",
    "        if pool.empty:\n",
    "            continue\n",
    "        sample_size = min(len(pool), quota)\n",
    "        if sample_size == 0:\n",
    "            continue\n",
    "        selections.append(\n",
    "            pool.sample(n=sample_size, random_state=rng.randint(0, 10**6))\n",
    "        )\n",
    "    if not selections:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(selections, ignore_index=True)\n",
    "\n",
    "\n",
    "FEW_SHOT_SPEC = {\"PASS\": 2, \"FAIL\": 3}\n",
    "\n",
    "few_shot_examples = sample_few_shot_examples(\n",
    "    splits[\"train\"], \"human_judgement\", FEW_SHOT_SPEC, seed=RANDOM_SEED\n",
    ")\n",
    "few_shot_examples[[\"email_id\", \"human_judgement\", \"summary\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_example_block(row: pd.Series) -> str:\n",
    "    return dedent(\n",
    "        f\"\"\"\n",
    "        ### Example ({row['human_judgement']})\n",
    "        Email:\n",
    "        {row['email']}\n",
    "\n",
    "        Generated Summary:\n",
    "        {row['summary']}\n",
    "\n",
    "        Human Rationale:\n",
    "        {row['human_reasoning']}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "BASE_PROMPT_HEADER = dedent(\n",
    "    \"\"\"\n",
    "    You are an expert executive-communication editor judging whether a model-produced email summary preserves logical coherence with the source email.\n",
    "\n",
    "    Definitions:\n",
    "    - PASS: The summary follows the email's chronology, keeps the cause-and-effect relationships intact, and does not contradict or omit key decisions or next steps.\n",
    "    - FAIL: The summary scrambles the story, breaks causal links, introduces contradictions, or drops essential commitments.\n",
    "\n",
    "    Use the examples below to anchor your decisions. Each example includes the original email, the model's summary, and why a human labeled it PASS or FAIL.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "example_blocks = \"\\n\\n\".join(\n",
    "    render_example_block(row)\n",
    "    for _, row in few_shot_examples.iterrows()\n",
    ")\n",
    "\n",
    "judge_prompt_template = dedent(\n",
    "    f\"\"\"\n",
    "    {BASE_PROMPT_HEADER}\n",
    "\n",
    "    {example_blocks}\n",
    "\n",
    "    Now evaluate the candidate summary below for logical coherence.\n",
    "\n",
    "    Email:\n",
    "    __EMAIL__\n",
    "\n",
    "    Model Summary:\n",
    "    __SUMMARY__\n",
    "\n",
    "    Respond in JSON with keys \"reasoning\" and \"label\" (either \"PASS\" or \"FAIL\").\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "print(judge_prompt_template[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31c432",
   "metadata": {},
   "source": [
    "### Judge Agent (Pydantic AI)\n",
    "Instantiate a structured-output agent that produces `reasoning` and `label` fields when we feed the prompt above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba22fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "try:\n",
    "    from pydantic_ai import Agent\n",
    "    from pydantic_ai.exceptions import UnexpectedModelBehavior\n",
    "except ModuleNotFoundError:\n",
    "    Agent = None\n",
    "    UnexpectedModelBehavior = Exception\n",
    "    print(\"Install pydantic-ai (pip install pydantic-ai) to enable judge calls.\")\n",
    "\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    label: Literal[\"PASS\", \"FAIL\"]\n",
    "\n",
    "\n",
    "if Agent is not None:\n",
    "    JUDGE_MODEL = os.getenv(\"JUDGE_MODEL\", \"gpt-5-mini\")\n",
    "    judge_agent = Agent(\n",
    "        JUDGE_MODEL,\n",
    "        system_prompt=\"You are an email summarization evaluator focused on logical coherence.\",\n",
    "    )\n",
    "    print(f\"Judge agent ready with model: {JUDGE_MODEL}\")\n",
    "else:\n",
    "    JUDGE_MODEL = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16c336",
   "metadata": {},
   "source": [
    "### Batch Scoring Helpers\n",
    "We parallelize judging with a `ThreadPoolExecutor` so the notebook can score 20-row batches quickly while still capturing structured outputs for accuracy checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21071c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def render_prompt(prompt_template: str, email_text: str, summary_text: str) -> str:\n",
    "    return (\n",
    "        prompt_template\n",
    "        .replace(\"__EMAIL__\", email_text.strip())\n",
    "        .replace(\"__SUMMARY__\", summary_text.strip())\n",
    "    )\n",
    "\n",
    "\n",
    "def _judge_single(agent: Any, prompt_template: str, row: dict) -> dict:\n",
    "    prompt = render_prompt(prompt_template, row[\"email\"], row[\"summary\"])\n",
    "    try:\n",
    "        run = agent.run_sync(prompt, output_type=JudgeOutput)\n",
    "        predicted_label = run.output.label\n",
    "        reasoning = run.output.reasoning\n",
    "    except UnexpectedModelBehavior as exc:\n",
    "        predicted_label = \"ERROR\"\n",
    "        reasoning = f\"Schema mismatch: {exc}\"\n",
    "    except Exception as exc:\n",
    "        predicted_label = \"ERROR\"\n",
    "        reasoning = str(exc)\n",
    "\n",
    "    return {\n",
    "        \"email_id\": row.get(\"email_id\"),\n",
    "        \"human_label\": row.get(\"human_judgement\"),\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"reasoning\": reasoning,\n",
    "    }\n",
    "\n",
    "\n",
    "async def score_rows(\n",
    "    agent: Any,\n",
    "    prompt_template: str,\n",
    "    rows: List[dict],\n",
    "    *,\n",
    "    max_workers: int = 8,\n",
    ") -> pd.DataFrame:\n",
    "    if Agent is None:\n",
    "        raise RuntimeError(\"Install pydantic-ai to score summaries.\")\n",
    "\n",
    "    loop = asyncio.get_running_loop()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        tasks = [\n",
    "            loop.run_in_executor(executor, _judge_single, agent, prompt_template, row)\n",
    "            for row in rows\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compute_accuracy(results: pd.DataFrame) -> Dict[str, float]:\n",
    "    mask = results[\"predicted_label\"].isin({\"PASS\", \"FAIL\"})\n",
    "    evaluated = results.loc[mask]\n",
    "\n",
    "    total = len(evaluated)\n",
    "    if total == 0:\n",
    "        return {\n",
    "            \"coverage\": 0.0,\n",
    "            \"records_evaluated\": 0,\n",
    "            \"accuracy\": float(\"nan\"),\n",
    "            \"tpr\": float(\"nan\"),\n",
    "            \"tnr\": float(\"nan\"),\n",
    "            \"balanced_accuracy\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    tp = ((evaluated[\"human_label\"] == \"PASS\") & (evaluated[\"predicted_label\"] == \"PASS\")).sum()\n",
    "    tn = ((evaluated[\"human_label\"] == \"FAIL\") & (evaluated[\"predicted_label\"] == \"FAIL\")).sum()\n",
    "    fp = ((evaluated[\"human_label\"] == \"FAIL\") & (evaluated[\"predicted_label\"] == \"PASS\")).sum()\n",
    "    fn = ((evaluated[\"human_label\"] == \"PASS\") & (evaluated[\"predicted_label\"] == \"FAIL\")).sum()\n",
    "\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else float(\"nan\")\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else float(\"nan\")\n",
    "    accuracy = (tp + tn) / total\n",
    "    balanced = (tpr + tnr) / 2 if not (pd.isna(tpr) or pd.isna(tnr)) else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"coverage\": len(evaluated) / len(results) if len(results) else 0.0,\n",
    "        \"records_evaluated\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"balanced_accuracy\": balanced,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ff2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Agent is not None:\n",
    "    sample_rows = (\n",
    "        splits[\"val\"]\n",
    "        .sample(n=min(20, len(splits[\"val\"])), random_state=RANDOM_SEED)\n",
    "        .to_dict(\"records\")\n",
    "    )\n",
    "    # Run this cell after configuring API credentials for the selected model.\n",
    "    preview = await score_rows(judge_agent, judge_prompt_template, sample_rows, max_workers=8)\n",
    "    display(preview)\n",
    "else:\n",
    "    print(\"Install pydantic-ai and instantiate `judge_agent` before scoring.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800277d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_accuracy(preview)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d8300",
   "metadata": {},
   "source": [
    "## HW\n",
    " - Experimenet with different few-shot examples to see how they impact judge accuracy on the validation set.\n",
    " - Experiment with prompt wording to see how it impacts judge accuracy on the validation set.\n",
    " - Optional: Use DSPY optimizers to find the best combination of few-shot examples and prompt wording to maximize judge accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ceddc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
