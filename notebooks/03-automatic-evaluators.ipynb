{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 · Automated Evaluators for the Email Summarizer\n",
    "\n",
    "This notebook sits in the **Measure** phase of the workshop. Earlier notebooks surfaced failure modes and produced labeled traces. Here we explain why automated evaluators matter, compare reference-free and reference-based approaches for the email summarizer, and show how programmatic checks and LLM-as-judge workflows accelerate iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Automated Evaluators?\n",
    "- Manual re-labeling after every prompt tweak is slow and inconsistent. Automated evaluators let us re-measure a new run of summaries in minutes rather than hours.\n",
    "- The Analyze → Measure → Improve loop depends on trustworthy metrics. Code or LLM judges give us repeatable estimates of how often the summarizer still fails.\n",
    "- Automation is especially valuable for the Enron email summarizer because many prompts need small wording changes; we do not want to hand-label the same trace set after every tweak.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca45ef9",
   "metadata": {},
   "source": [
    "## Reference-Free vs Reference-Based Metrics\n",
    "Reference-free metrics inspect the model output directly, while reference-based metrics compare it to a trusted target (for example, human-written summary bullets). In practice we stack both to cover different failure surfaces.\n",
    "\n",
    "| Metric Type        | Summarizer Example                                        | What It Checks                                | Strengths                                   | Considerations |\n",
    "|--------------------|-----------------------------------------------------------|------------------------------------------------|---------------------------------------------|----------------|\n",
    "| Reference-Free     | Flag summaries that sound too casual for executive readers | Tone, structure, presence of a CTA             | Cheap, deterministic, easy to debug         | Misses nuanced factual errors |\n",
    "| Reference-Based    | Compare generated bullets to analyst-written gold bullets  | Coverage of key decisions / action items       | Precise fidelity check                       | Needs curated references per email |\n",
    "| Hybrid             | Require both tone check and coverage check                | Tone + correctness in one report               | Broader failure coverage                     | Higher maintenance across metrics |\n",
    "\n",
    "The rest of this notebook uses synthetic data shaped like our email summarizer traces to illustrate each style of evaluator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bf97b",
   "metadata": {},
   "source": [
    "## Programmatic Evaluators for Informal Tone\n",
    "When the failure definition is objective (e.g., \"+/-\" on informal phrases), a programmatic evaluator is fast and reliable. Below we flag summaries that include slang we have agreed is unacceptable for client-facing communications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbad88a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>flagged_informal</th>\n",
       "      <th>informal_hits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S-001</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Human Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S-002</td>\n",
       "      <td>True</td>\n",
       "      <td>[hey team, you guys, super pumped]</td>\n",
       "      <td>Human Fail (too casual)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S-003</td>\n",
       "      <td>True</td>\n",
       "      <td>[cheers]</td>\n",
       "      <td>Human Fail (casual sign-off)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S-004</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Human Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary_id  flagged_informal                       informal_hits  \\\n",
       "0      S-001             False                                  []   \n",
       "1      S-002              True  [hey team, you guys, super pumped]   \n",
       "2      S-003              True                            [cheers]   \n",
       "3      S-004             False                                  []   \n",
       "\n",
       "                          label  \n",
       "0                    Human Pass  \n",
       "1       Human Fail (too casual)  \n",
       "2  Human Fail (casual sign-off)  \n",
       "3                    Human Pass  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INFORMAL_KEYWORDS = {\n",
    "    \"hey team\",\n",
    "    \"super pumped\",\n",
    "    \"lol\",\n",
    "    \"cheers\",\n",
    "    \"you guys\",\n",
    "    \"gonna\",\n",
    "}\n",
    "\n",
    "sample_summaries = pd.DataFrame([\n",
    "    {\n",
    "        \"summary_id\": \"S-001\",\n",
    "        \"summary\": \"Team — Here's the recap: Decisions were documented, and Finance will review numbers tomorrow. No informal tone here.\",\n",
    "        \"label\": \"Human Pass\",\n",
    "    },\n",
    "    {\n",
    "        \"summary_id\": \"S-002\",\n",
    "        \"summary\": \"Hey team! Super pumped about the vendor shortlist. You guys should ping me if anything feels off.\",\n",
    "        \"label\": \"Human Fail (too casual)\",\n",
    "    },\n",
    "    {\n",
    "        \"summary_id\": \"S-003\",\n",
    "        \"summary\": \"The group confirmed the migration timeline. Cheers, and let's lock in the training invites.\",\n",
    "        \"label\": \"Human Fail (casual sign-off)\",\n",
    "    },\n",
    "    {\n",
    "        \"summary_id\": \"S-004\",\n",
    "        \"summary\": \"Team — All action items remain with Ops; no tone violations were spotted.\",\n",
    "        \"label\": \"Human Pass\",\n",
    "    },\n",
    "])\n",
    "\n",
    "\n",
    "def detect_informal_tone(text: str):\n",
    "    lowered = text.lower()\n",
    "    hits = [kw for kw in INFORMAL_KEYWORDS if kw in lowered]\n",
    "    return hits\n",
    "\n",
    "\n",
    "sample_summaries[\"informal_hits\"] = sample_summaries[\"summary\"].apply(detect_informal_tone)\n",
    "sample_summaries[\"flagged_informal\"] = sample_summaries[\"informal_hits\"].apply(bool)\n",
    "sample_summaries[[\"summary_id\", \"flagged_informal\", \"informal_hits\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c6c8d",
   "metadata": {},
   "source": [
    "The heuristic catches rows S-002 and S-003 because they contain agreed-upon informal phrases. Programmatic evaluators like this are ideal for specification failures we can encode as deterministic rules.\n",
    "\n",
    "For nuanced behaviors such as “Did the summary capture every decision from the thread without hallucinating?” we lean on an LLM-as-judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72e885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "405f251b",
   "metadata": {},
   "source": [
    "### Dataset Setup and Stratified Splits\n",
    "We'll start from the merged human-labeled set `data/llm-judge-sample-full.json` and carve out 15%/40%/45% train/validation/test splits while preserving the pass/fail balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028489db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAIL</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASS</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  count\n",
       "0  FAIL     41\n",
       "1  PASS     34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_PATH = Path(\"../data/llm-judge-sample-full.json\")\n",
    "\n",
    "judge_df = pd.read_json(DATA_PATH)\n",
    "judge_df[\"human_judgement\"] = judge_df[\"human_judgement\"].str.upper()\n",
    "\n",
    "label_counts = (\n",
    "    judge_df[\"human_judgement\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"label\")\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "label_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5c9e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAIL</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASS</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train  val  test\n",
       "label                  \n",
       "FAIL       6   16    19\n",
       "PASS       5   14    15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_FRACTIONS = (0.15, 0.40, 0.45)\n",
    "\n",
    "splits = stratified_split_sklearn(\n",
    "    judge_df,\n",
    "    label_col=\"human_judgement\",\n",
    "    fractions=SPLIT_FRACTIONS,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "split_summary = (\n",
    "    pd.concat(\n",
    "        {\n",
    "            split_name: df[\"human_judgement\"].value_counts()\n",
    "            for split_name, df in splits.items()\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .rename_axis(\"label\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "split_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de479c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       train  val  test\n",
      "label                  \n",
      "FAIL       6   16    19\n",
      "PASS       5   14    15\n",
      "Wrote 11 rows to ../data/llm-judge-split-train.json\n",
      "Wrote 30 rows to ../data/llm-judge-split-val.json\n",
      "Wrote 34 rows to ../data/llm-judge-split-test.json\n"
     ]
    }
   ],
   "source": [
    "SPLIT_FRACTIONS = (0.15, 0.40, 0.45)\n",
    "\n",
    "# --- Step 1: Split using sklearn-based stratified split ---\n",
    "splits = stratified_split_sklearn(\n",
    "    judge_df,\n",
    "    label_col=\"human_judgement\",\n",
    "    fractions=SPLIT_FRACTIONS,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# --- Step 2: Check split summary (unchanged) ---\n",
    "split_summary = (\n",
    "    pd.concat(\n",
    "        {\n",
    "            split_name: df[\"human_judgement\"].value_counts()\n",
    "            for split_name, df in splits.items()\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .rename_axis(\"label\")\n",
    "    .sort_index()\n",
    ")\n",
    "print(split_summary)\n",
    "\n",
    "# --- Step 3: Write splits to JSON files (unchanged) ---\n",
    "from pathlib import Path\n",
    "\n",
    "for split_name, df in splits.items():\n",
    "    output_path = Path(f\"../data/llm-judge-split-{split_name}.json\")\n",
    "    output_path.write_text(df.to_json(orient=\"records\", indent=2))\n",
    "    print(f\"Wrote {len(df)} rows to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b21e1",
   "metadata": {},
   "source": [
    "### Few-shot Prompt Builder\n",
    "Select a handful of pass/fail examples from the train split so the judge can learn what logically coherent summaries look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77b8e633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>human_judgement</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>PASS</td>\n",
       "      <td>Steven reviewed Q3 customer surveys revealing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>PASS</td>\n",
       "      <td>To preserve EBITDA while covering Engineering’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>They lost $500K from outages and the CEO calle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>RTO was met at 3:38 and RPO within 5 minutes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The team is delaying GA to October 18 to allow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id human_judgement                                            summary\n",
       "0        10            PASS  Steven reviewed Q3 customer surveys revealing ...\n",
       "1        29            PASS  To preserve EBITDA while covering Engineering’...\n",
       "2        18            FAIL  They lost $500K from outages and the CEO calle...\n",
       "3        48            FAIL  RTO was met at 3:38 and RPO within 5 minutes a...\n",
       "4        30            FAIL  The team is delaying GA to October 18 to allow..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from textwrap import dedent\n",
    "import random\n",
    "\n",
    "\n",
    "def sample_few_shot_examples(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str,\n",
    "    per_label: dict,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> pd.DataFrame:\n",
    "    rng = random.Random(seed)\n",
    "    selections = []\n",
    "    for label, quota in per_label.items():\n",
    "        pool = df[df[label_col] == label]\n",
    "        if pool.empty:\n",
    "            continue\n",
    "        sample_size = min(len(pool), quota)\n",
    "        if sample_size == 0:\n",
    "            continue\n",
    "        selections.append(\n",
    "            pool.sample(n=sample_size, random_state=rng.randint(0, 10**6))\n",
    "        )\n",
    "    if not selections:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(selections, ignore_index=True)\n",
    "\n",
    "\n",
    "FEW_SHOT_SPEC = {\"PASS\": 2, \"FAIL\": 3}\n",
    "\n",
    "few_shot_examples = sample_few_shot_examples(\n",
    "    splits[\"train\"], \"human_judgement\", FEW_SHOT_SPEC, seed=RANDOM_SEED\n",
    ")\n",
    "few_shot_examples[[\"email_id\", \"human_judgement\", \"summary\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5224fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert executive-communication editor judging whether a model-produced email summary preserves logical coherence with the source email.\n",
      "\n",
      "Definitions:\n",
      "- PASS: The summary follows the email's chronology, keeps the cause-and-effect relationships intact, and does not contradict or omit key decisions or next steps.\n",
      "- FAIL: The summary scrambles the story, breaks causal links, introduces contradictions, or drops essential commitments.\n",
      "\n",
      "Use the examples below to anchor your decisions. Each example includes the original email, the model's summary, and why a human labeled it PASS or FAIL.\n",
      "\n",
      "    ### Example (PASS)\n",
      "        Email:\n",
      "        Hi Patricia,\n",
      "\n",
      "I've reviewed the customer satisfaction surveys from Q3, and I have both good and concerning news.\n",
      "\n",
      "The positive: Our Net Promoter Score increased to 72 (up from 65 in Q2), and product quality ratings are at an all-time high of 4.7/5.\n",
      "\n",
      "However, customer support satisfaction dropped to 3.2/5. The main complaints were:\n",
      "- Long wait times (ave\n"
     ]
    }
   ],
   "source": [
    "def render_example_block(row: pd.Series) -> str:\n",
    "    return dedent(\n",
    "        f\"\"\"\n",
    "        ### Example ({row['human_judgement']})\n",
    "        Email:\n",
    "        {row['email']}\n",
    "\n",
    "        Generated Summary:\n",
    "        {row['summary']}\n",
    "\n",
    "        Human Rationale:\n",
    "        {row['human_reasoning']}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "BASE_PROMPT_HEADER = dedent(\n",
    "    \"\"\"\n",
    "    You are an expert executive-communication editor judging whether a model-produced email summary preserves logical coherence with the source email.\n",
    "\n",
    "    Definitions:\n",
    "    - PASS: The summary follows the email's chronology, keeps the cause-and-effect relationships intact, and does not contradict or omit key decisions or next steps.\n",
    "    - FAIL: The summary scrambles the story, breaks causal links, introduces contradictions, or drops essential commitments.\n",
    "\n",
    "    Use the examples below to anchor your decisions. Each example includes the original email, the model's summary, and why a human labeled it PASS or FAIL.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "example_blocks = \"\\n\\n\".join(\n",
    "    render_example_block(row)\n",
    "    for _, row in few_shot_examples.iterrows()\n",
    ")\n",
    "\n",
    "judge_prompt_template = dedent(\n",
    "    f\"\"\"\n",
    "    {BASE_PROMPT_HEADER}\n",
    "\n",
    "    {example_blocks}\n",
    "\n",
    "    Now evaluate the candidate summary below for logical coherence.\n",
    "\n",
    "    Email:\n",
    "    __EMAIL__\n",
    "\n",
    "    Model Summary:\n",
    "    __SUMMARY__\n",
    "\n",
    "    Respond in JSON with keys \"reasoning\" and \"label\" (either \"PASS\" or \"FAIL\").\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "print(judge_prompt_template[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31c432",
   "metadata": {},
   "source": [
    "### Judge Agent (Pydantic AI)\n",
    "Instantiate a structured-output agent that produces `reasoning` and `label` fields when we feed the prompt above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cba22fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge agent ready with model: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "try:\n",
    "    from pydantic_ai import Agent\n",
    "    from pydantic_ai.exceptions import UnexpectedModelBehavior\n",
    "except ModuleNotFoundError:\n",
    "    Agent = None\n",
    "    UnexpectedModelBehavior = Exception\n",
    "    print(\"Install pydantic-ai (pip install pydantic-ai) to enable judge calls.\")\n",
    "\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    label: Literal[\"PASS\", \"FAIL\"]\n",
    "\n",
    "\n",
    "if Agent is not None:\n",
    "    JUDGE_MODEL = os.getenv(\"JUDGE_MODEL\", \"gpt-5-mini\")\n",
    "    judge_agent = Agent(\n",
    "        JUDGE_MODEL,\n",
    "        system_prompt=\"You are an email summarization evaluator focused on logical coherence.\",\n",
    "    )\n",
    "    print(f\"Judge agent ready with model: {JUDGE_MODEL}\")\n",
    "else:\n",
    "    JUDGE_MODEL = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16c336",
   "metadata": {},
   "source": [
    "### Batch Scoring Helpers\n",
    "We parallelize judging with a `ThreadPoolExecutor` so the notebook can score 20-row batches quickly while still capturing structured outputs for accuracy checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21071c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def render_prompt(prompt_template: str, email_text: str, summary_text: str) -> str:\n",
    "    return (\n",
    "        prompt_template\n",
    "        .replace(\"__EMAIL__\", email_text.strip())\n",
    "        .replace(\"__SUMMARY__\", summary_text.strip())\n",
    "    )\n",
    "\n",
    "\n",
    "def _judge_single(agent: Any, prompt_template: str, row: dict) -> dict:\n",
    "    prompt = render_prompt(prompt_template, row[\"email\"], row[\"summary\"])\n",
    "    try:\n",
    "        run = agent.run_sync(prompt, output_type=JudgeOutput)\n",
    "        predicted_label = run.output.label\n",
    "        reasoning = run.output.reasoning\n",
    "    except UnexpectedModelBehavior as exc:\n",
    "        predicted_label = \"ERROR\"\n",
    "        reasoning = f\"Schema mismatch: {exc}\"\n",
    "    except Exception as exc:\n",
    "        predicted_label = \"ERROR\"\n",
    "        reasoning = str(exc)\n",
    "\n",
    "    return {\n",
    "        \"email_id\": row.get(\"email_id\"),\n",
    "        \"human_label\": row.get(\"human_judgement\"),\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"reasoning\": reasoning,\n",
    "    }\n",
    "\n",
    "\n",
    "async def score_rows(\n",
    "    agent: Any,\n",
    "    prompt_template: str,\n",
    "    rows: List[dict],\n",
    "    *,\n",
    "    max_workers: int = 8,\n",
    ") -> pd.DataFrame:\n",
    "    if Agent is None:\n",
    "        raise RuntimeError(\"Install pydantic-ai to score summaries.\")\n",
    "\n",
    "    loop = asyncio.get_running_loop()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        tasks = [\n",
    "            loop.run_in_executor(executor, _judge_single, agent, prompt_template, row)\n",
    "            for row in rows\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compute_accuracy(results: pd.DataFrame) -> Dict[str, float]:\n",
    "    mask = results[\"predicted_label\"].isin({\"PASS\", \"FAIL\"})\n",
    "    evaluated = results.loc[mask]\n",
    "\n",
    "    total = len(evaluated)\n",
    "    if total == 0:\n",
    "        return {\n",
    "            \"coverage\": 0.0,\n",
    "            \"records_evaluated\": 0,\n",
    "            \"accuracy\": float(\"nan\"),\n",
    "            \"tpr\": float(\"nan\"),\n",
    "            \"tnr\": float(\"nan\"),\n",
    "            \"balanced_accuracy\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    tp = ((evaluated[\"human_label\"] == \"PASS\") & (evaluated[\"predicted_label\"] == \"PASS\")).sum()\n",
    "    tn = ((evaluated[\"human_label\"] == \"FAIL\") & (evaluated[\"predicted_label\"] == \"FAIL\")).sum()\n",
    "    fp = ((evaluated[\"human_label\"] == \"FAIL\") & (evaluated[\"predicted_label\"] == \"PASS\")).sum()\n",
    "    fn = ((evaluated[\"human_label\"] == \"PASS\") & (evaluated[\"predicted_label\"] == \"FAIL\")).sum()\n",
    "\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else float(\"nan\")\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else float(\"nan\")\n",
    "    accuracy = (tp + tn) / total\n",
    "    balanced = (tpr + tnr) / 2 if not (pd.isna(tpr) or pd.isna(tnr)) else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"coverage\": len(evaluated) / len(results) if len(results) else 0.0,\n",
    "        \"records_evaluated\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"balanced_accuracy\": balanced,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "451ff2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/dyw0vdnx2jg9lyq8m01n8nfm0000gn/T/ipykernel_26129/1526188403.py:8: RuntimeWarning: coroutine 'score_rows' was never awaited\n",
      "  preview = await score_rows(judge_agent, judge_prompt_template, sample_rows, max_workers=20)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>human_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary accurately preserves the ema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary accurately mirrors the email...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The model summary accurately captures the key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary accurately reflects the emai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary accurately preserves the ema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The model summary includes all key facts (elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The summary accurately captures the email’s fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary accurately preserves the ema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The summary accurately preserves the email’s c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary preserves the email’s chrono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>54</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The summary captures the decision to move, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary preserves all key facts and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The summary introduces two key logical errors ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The summary preserves the email’s chronologica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary faithfully preserves the ema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary accurately captures the emai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>42</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The model summary accurately captures the key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The summary faithfully mirrors the email’s seq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>The summary includes many correct facts (launc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>PASS</td>\n",
       "      <td>The model summary faithfully preserves the ema...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    email_id human_label predicted_label  \\\n",
       "0         16        FAIL            PASS   \n",
       "1         67        PASS            PASS   \n",
       "2         40        FAIL            FAIL   \n",
       "3          8        PASS            PASS   \n",
       "4         39        PASS            PASS   \n",
       "5         58        FAIL            FAIL   \n",
       "6         34        FAIL            FAIL   \n",
       "7         15        PASS            PASS   \n",
       "8          4        PASS            PASS   \n",
       "9         25        PASS            PASS   \n",
       "10        54        FAIL            FAIL   \n",
       "11        11        FAIL            PASS   \n",
       "12        52        FAIL            FAIL   \n",
       "13        33        PASS            PASS   \n",
       "14        14        PASS            PASS   \n",
       "15         2        PASS            PASS   \n",
       "16        42        FAIL            FAIL   \n",
       "17        24        PASS            PASS   \n",
       "18        26        FAIL            FAIL   \n",
       "19        13        FAIL            PASS   \n",
       "\n",
       "                                            reasoning  \n",
       "0   The model summary accurately preserves the ema...  \n",
       "1   The model summary accurately mirrors the email...  \n",
       "2   The model summary accurately captures the key ...  \n",
       "3   The model summary accurately reflects the emai...  \n",
       "4   The model summary accurately preserves the ema...  \n",
       "5   The model summary includes all key facts (elec...  \n",
       "6   The summary accurately captures the email’s fa...  \n",
       "7   The model summary accurately preserves the ema...  \n",
       "8   The summary accurately preserves the email’s c...  \n",
       "9   The model summary preserves the email’s chrono...  \n",
       "10  The summary captures the decision to move, the...  \n",
       "11  The model summary preserves all key facts and ...  \n",
       "12  The summary introduces two key logical errors ...  \n",
       "13  The summary preserves the email’s chronologica...  \n",
       "14  The model summary faithfully preserves the ema...  \n",
       "15  The model summary accurately captures the emai...  \n",
       "16  The model summary accurately captures the key ...  \n",
       "17  The summary faithfully mirrors the email’s seq...  \n",
       "18  The summary includes many correct facts (launc...  \n",
       "19  The model summary faithfully preserves the ema...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Agent is not None:\n",
    "    sample_rows = (\n",
    "        splits[\"val\"]\n",
    "        .sample(n=min(20, len(splits[\"val\"])), random_state=RANDOM_SEED)\n",
    "        .to_dict(\"records\")\n",
    "    )\n",
    "    # Run this cell after configuring API credentials for the selected model.\n",
    "    preview = await score_rows(judge_agent, judge_prompt_template, sample_rows, max_workers=8)\n",
    "    display(preview)\n",
    "else:\n",
    "    print(\"Install pydantic-ai and instantiate `judge_agent` before scoring.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "800277d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 1.0,\n",
       " 'records_evaluated': 20,\n",
       " 'accuracy': np.float64(0.85),\n",
       " 'tpr': np.float64(1.0),\n",
       " 'tnr': np.float64(0.7),\n",
       " 'balanced_accuracy': np.float64(0.85)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = compute_accuracy(preview)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d8300",
   "metadata": {},
   "source": [
    "## HW\n",
    " - Experimenet with different few-shot examples to see how they impact judge accuracy on the validation set.\n",
    " - Experiment with prompt wording to see how it impacts judge accuracy on the validation set.\n",
    " - Optional: Use DSPY optimizers to find the best combination of few-shot examples and prompt wording to maximize judge accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ceddc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
