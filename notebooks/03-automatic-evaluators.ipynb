{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 · Automated Evaluators for the Email Summarizer\n",
        "\n",
        "This notebook sits in the **Measure** phase of the workshop. Earlier notebooks surfaced failure modes and produced labeled traces. Here we explain why automated evaluators matter, compare reference-free and reference-based approaches for the email summarizer, and show how programmatic checks and LLM-as-judge workflows accelerate iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Automated Evaluators?\n",
        "- Manual re-labeling after every prompt tweak is slow and inconsistent. Automated evaluators let us re-measure a new run of summaries in minutes rather than hours.\n",
        "- The Analyze → Measure → Improve loop depends on trustworthy metrics. Code or LLM judges give us repeatable estimates of how often the summarizer still fails.\n",
        "- Automation is especially valuable for the Enron email summarizer because many prompts need small wording changes; we do not want to hand-label the same trace set after every tweak.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reference-Free vs Reference-Based Metrics\n",
        "Reference-free metrics inspect the model output directly, while reference-based metrics compare it to a trusted target (for example, human-written summary bullets). In practice we stack both to cover different failure surfaces.\n",
        "\n",
        "| Metric Type        | Summarizer Example                                        | What It Checks                                | Strengths                                   | Considerations |\n",
        "|--------------------|-----------------------------------------------------------|------------------------------------------------|---------------------------------------------|----------------|\n",
        "| Reference-Free     | Flag summaries that sound too casual for executive readers | Tone, structure, presence of a CTA             | Cheap, deterministic, easy to debug         | Misses nuanced factual errors |\n",
        "| Reference-Based    | Compare generated bullets to analyst-written gold bullets  | Coverage of key decisions / action items       | Precise fidelity check                       | Needs curated references per email |\n",
        "| Hybrid             | Require both tone check and coverage check                | Tone + correctness in one report               | Broader failure coverage                     | Higher maintenance across metrics |\n",
        "\n",
        "The rest of this notebook uses synthetic data shaped like our email summarizer traces to illustrate each style of evaluator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Programmatic Evaluators for Informal Tone\n",
        "When the failure definition is objective (e.g., \"+/-\" on informal phrases), a programmatic evaluator is fast and reliable. Below we flag summaries that include slang we have agreed is unacceptable for client-facing communications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary_id</th>\n",
              "      <th>flagged_informal</th>\n",
              "      <th>informal_hits</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S-001</td>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>Human Pass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S-002</td>\n",
              "      <td>True</td>\n",
              "      <td>[hey team, you guys, super pumped]</td>\n",
              "      <td>Human Fail (too casual)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S-003</td>\n",
              "      <td>True</td>\n",
              "      <td>[cheers]</td>\n",
              "      <td>Human Fail (casual sign-off)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S-004</td>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>Human Pass</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  summary_id  flagged_informal                       informal_hits  \\\n",
              "0      S-001             False                                  []   \n",
              "1      S-002              True  [hey team, you guys, super pumped]   \n",
              "2      S-003              True                            [cheers]   \n",
              "3      S-004             False                                  []   \n",
              "\n",
              "                          label  \n",
              "0                    Human Pass  \n",
              "1       Human Fail (too casual)  \n",
              "2  Human Fail (casual sign-off)  \n",
              "3                    Human Pass  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "INFORMAL_KEYWORDS = {\n",
        "    \"hey team\",\n",
        "    \"super pumped\",\n",
        "    \"lol\",\n",
        "    \"cheers\",\n",
        "    \"you guys\",\n",
        "    \"gonna\",\n",
        "}\n",
        "\n",
        "sample_summaries = pd.DataFrame([\n",
        "    {\n",
        "        \"summary_id\": \"S-001\",\n",
        "        \"summary\": \"Team — Here's the recap: Decisions were documented, and Finance will review numbers tomorrow. No informal tone here.\",\n",
        "        \"label\": \"Human Pass\",\n",
        "    },\n",
        "    {\n",
        "        \"summary_id\": \"S-002\",\n",
        "        \"summary\": \"Hey team! Super pumped about the vendor shortlist. You guys should ping me if anything feels off.\",\n",
        "        \"label\": \"Human Fail (too casual)\",\n",
        "    },\n",
        "    {\n",
        "        \"summary_id\": \"S-003\",\n",
        "        \"summary\": \"The group confirmed the migration timeline. Cheers, and let's lock in the training invites.\",\n",
        "        \"label\": \"Human Fail (casual sign-off)\",\n",
        "    },\n",
        "    {\n",
        "        \"summary_id\": \"S-004\",\n",
        "        \"summary\": \"Team — All action items remain with Ops; no tone violations were spotted.\",\n",
        "        \"label\": \"Human Pass\",\n",
        "    },\n",
        "])\n",
        "\n",
        "\n",
        "def detect_informal_tone(text: str):\n",
        "    lowered = text.lower()\n",
        "    hits = [kw for kw in INFORMAL_KEYWORDS if kw in lowered]\n",
        "    return hits\n",
        "\n",
        "\n",
        "sample_summaries[\"informal_hits\"] = sample_summaries[\"summary\"].apply(detect_informal_tone)\n",
        "sample_summaries[\"flagged_informal\"] = sample_summaries[\"informal_hits\"].apply(bool)\n",
        "sample_summaries[[\"summary_id\", \"flagged_informal\", \"informal_hits\", \"label\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The heuristic catches rows S-002 and S-003 because they contain agreed-upon informal phrases. Programmatic evaluators like this are ideal for specification failures we can encode as deterministic rules.\n",
        "\n",
        "For nuanced behaviors such as “Did the summary capture every decision from the thread without hallucinating?” we lean on an LLM-as-judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af72e885",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM-as-Judge Evaluators\n",
        "### Why we need them\n",
        "- Capturing every decision/action item requires understanding context, synonyms, and implicature—hard to encode with pure string rules.\n",
        "- We want interpretable reasoning about misses so we can inspect disagreements.\n",
        "- LLM judges align with Chapter 5 guidance: give each failure mode a narrowly scoped Pass/Fail task, backed by few-shot examples and structured output.\n",
        "\n",
        "### Prompt anatomy for the summarizer (Decision Coverage criterion)\n",
        "1. **Task framing** – “Decide if the summary captures all committed decisions without casual tone.”\n",
        "2. **Definitions** – Spell out what counts as a Pass vs Fail, including partial coverage and tone violations.\n",
        "3. **Few-shot examples** – Pull from the labeled email summaries (train split only).\n",
        "4. **Structured output** – JSON with `reasoning` and `answer` so downstream code can parse and log.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Skeleton\n",
        "```\n",
        "You are an evaluator for internal executive email summaries. Decide whether the summary captures every committed decision from the source email while keeping a professional tone.\n",
        "\n",
        "Definitions:\n",
        "- Pass: All decisions/action items from the source are present (paraphrases allowed) and the tone stays professional.\n",
        "- Fail: Any decision/action item is missing, hallucinated, or the tone slips into casual language.\n",
        "\n",
        "Few-shot examples (from train split):\n",
        "<example>\n",
        "Source decisions: [\"Share revised headcount plan\", \"Schedule budget review\"]\n",
        "Summary: \"Team — We confirmed the revised headcount plan and scheduled the budget review for Thursday.\"\n",
        "Label: Pass\n",
        "</example>\n",
        "<example>\n",
        "Source decisions: [\"Send updated financial model\"]\n",
        "Summary: \"Hey team! Super pumped to send the model tomorrow.\"\n",
        "Label: Fail (casual tone)\n",
        "</example>\n",
        "\n",
        "Return JSON: {\"reasoning\": \"...\", \"answer\": \"Pass\" or \"Fail\"}\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To keep this notebook self-contained we simulate both the labeled dataset and the judge behavior. In practice you would replace the stub with a real LLM call (e.g., `openai.responses`) and store the prompt template under `prompts/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(seed=7)\n",
        "\n",
        "ACTION_LIBRARY = [\n",
        "    (\"Schedule finance review call\", \"Set up a finance check-in\"),\n",
        "    (\"Share updated revenue snapshot\", \"Send the refreshed revenue numbers\"),\n",
        "    (\"Confirm vendor shortlist\", \"Lock in the shortlist of vendors\"),\n",
        "    (\"Publish migration timeline\", \"Circulate the migration timeline\"),\n",
        "    (\"Finalize legal sign-off\", \"Secure legal approval\"),\n",
        "    (\"Send onboarding packet\", \"Deliver the onboarding materials\"),\n",
        "]\n",
        "\n",
        "INFORMAL_SNIPPETS = [\n",
        "    \"Hey team — quick blast!\",\n",
        "    \"Super pumped about this!\",\n",
        "    \"You guys crushed it\",\n",
        "    \"lol let's keep momentum\",\n",
        "    \"Cheers,\",\n",
        "]\n",
        "\n",
        "records = []\n",
        "for trace_id in range(1, 101):\n",
        "    action_count = int(rng.integers(2, 4))\n",
        "    action_indices = rng.choice(len(ACTION_LIBRARY), size=action_count, replace=False)\n",
        "    canonical_actions = [ACTION_LIBRARY[i][0] for i in action_indices]\n",
        "    paraphrase_actions = [ACTION_LIBRARY[i][1] for i in action_indices]\n",
        "\n",
        "    paraphrase_flag = bool(rng.random() < 0.4)\n",
        "    missed_action_flag = bool(rng.random() < 0.3)\n",
        "    informal_flag = bool(rng.random() < 0.2)\n",
        "    informal_intensity = float(rng.choice([0.6, 0.8, 1.0])) if informal_flag else 0.0\n",
        "\n",
        "    intro = \"Team — here is the recap from the thread.\"\n",
        "    informal_hits = []\n",
        "    if informal_flag:\n",
        "        snippet = rng.choice(INFORMAL_SNIPPETS)\n",
        "        intro = snippet\n",
        "        informal_hits.append(snippet.lower())\n",
        "\n",
        "    summary_actions = []\n",
        "    literal_matches = 0\n",
        "    paraphrased_count = 0\n",
        "    for canonical, paraphrase in zip(canonical_actions, paraphrase_actions):\n",
        "        text_choice = canonical\n",
        "        if paraphrase_flag and rng.random() < 0.6:\n",
        "            text_choice = paraphrase\n",
        "            paraphrased_count += 1\n",
        "        summary_actions.append(text_choice)\n",
        "        if text_choice == canonical:\n",
        "            literal_matches += 1\n",
        "\n",
        "    if missed_action_flag and len(summary_actions) > 1:\n",
        "        summary_actions.pop()\n",
        "\n",
        "    summary_body = \"Decisions:\n",
        "\" + \"\n",
        "\".join(f\"- {text}\" for text in summary_actions)\n",
        "    closing = \"We will revisit next sync for status checks.\"\n",
        "\n",
        "    summary_text = f\"{intro}\n",
        "\n",
        "{summary_body}\n",
        "\n",
        "{closing}\"\n",
        "\n",
        "    action_coverage = len(summary_actions) / len(canonical_actions)\n",
        "    literal_coverage = literal_matches / len(canonical_actions)\n",
        "    semantic_coverage = action_coverage\n",
        "    paraphrase_ratio = paraphrased_count / len(summary_actions) if summary_actions else 0.0\n",
        "\n",
        "    human_pass = not missed_action_flag and not informal_flag\n",
        "\n",
        "    records.append(\n",
        "        {\n",
        "            \"trace_id\": f\"T{trace_id:03d}\",\n",
        "            \"summary\": summary_text,\n",
        "            \"reference_actions\": canonical_actions,\n",
        "            \"summary_actions\": summary_actions,\n",
        "            \"action_coverage\": action_coverage,\n",
        "            \"literal_coverage\": literal_coverage,\n",
        "            \"semantic_coverage\": semantic_coverage,\n",
        "            \"paraphrase_ratio\": paraphrase_ratio,\n",
        "            \"paraphrase_flag\": paraphrase_flag,\n",
        "            \"missed_action_flag\": missed_action_flag,\n",
        "            \"informal_flag\": informal_flag,\n",
        "            \"informal_intensity\": informal_intensity,\n",
        "            \"informal_hits\": informal_hits,\n",
        "            \"human_pass\": human_pass,\n",
        "        }\n",
        "    )\n",
        "\n",
        "labeled_summaries = pd.DataFrame.from_records(records)\n",
        "\n",
        "# Introduce five borderline flips to mimic human ambiguity\n",
        "flip_indices = rng.choice(labeled_summaries.index, size=5, replace=False)\n",
        "labeled_summaries.loc[flip_indices, \"human_pass\"] = ~labeled_summaries.loc[flip_indices, \"human_pass\"]\n",
        "\n",
        "labeled_summaries.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now mimic the Chapter 5 discipline: carve out disjoint splits before writing any prompts.\n",
        "- **Train (15%)** – pool of clear Pass/Fail examples for few-shot snippets.\n",
        "- **Dev (40%)** – iterate here and inspect disagreements.\n",
        "- **Test (45%)** – untouched until we freeze the judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_rng = np.random.default_rng(seed=2024)\n",
        "indices = split_rng.permutation(len(labeled_summaries))\n",
        "\n",
        "train_size = 15\n",
        "dev_size = 40\n",
        "\n",
        "train_idx = indices[:train_size]\n",
        "dev_idx = indices[train_size:train_size + dev_size]\n",
        "test_idx = indices[train_size + dev_size:]\n",
        "\n",
        "splits = {\n",
        "    \"train\": labeled_summaries.iloc[train_idx].reset_index(drop=True),\n",
        "    \"dev\": labeled_summaries.iloc[dev_idx].reset_index(drop=True),\n",
        "    \"test\": labeled_summaries.iloc[test_idx].reset_index(drop=True),\n",
        "}\n",
        "\n",
        "{k: len(v) for k, v in splits.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The train split provides the few-shot snippets shown in the prompt skeleton. Below are three of them (redacted summaries shortened for readability).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_examples = splits[\"train\"][['trace_id', 'summary', 'human_pass']].head(3)\n",
        "train_examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating Judge Behaviour\n",
        "We use a heuristic stub to stand in for the LLM. It exposes the two core levers affected when we edit few-shot examples:\n",
        "- **Coverage sensitivity** – whether the judge recognises paraphrased decisions.\n",
        "- **Tone strictness** – how severely the judge penalises casual language.\n",
        "\n",
        "The baseline configuration mirrors a weak prompt that over-indexes on literal string matches and overlooks mild slang. The improved configuration reflects refined examples that highlight paraphrases and tone violations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "\n",
        "def run_judge(df: pd.DataFrame, *, coverage_column: str, coverage_threshold: float, informal_threshold: float, name: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for row in df.itertuples(index=False):\n",
        "        coverage_value = getattr(row, coverage_column)\n",
        "        coverage_ok = coverage_value >= coverage_threshold\n",
        "        coverage_reason = (\n",
        "            f\"Coverage {coverage_value:.0%} ≥ {coverage_threshold:.0%}\" if coverage_ok\n",
        "            else f\"Coverage {coverage_value:.0%} < {coverage_threshold:.0%}\"\n",
        "        )\n",
        "\n",
        "        tone_ok = True\n",
        "        tone_reason = \"Tone acceptable\"\n",
        "        if row.informal_intensity >= informal_threshold:\n",
        "            tone_ok = False\n",
        "            tone_reason = \"Informal tone flagged\"\n",
        "        elif row.informal_intensity > 0:\n",
        "            tone_reason = \"Mild casual tone allowed\"\n",
        "\n",
        "        passes = coverage_ok and tone_ok\n",
        "        rows.append(\n",
        "            {\n",
        "                \"trace_id\": row.trace_id,\n",
        "                \"judge_answer\": \"Pass\" if passes else \"Fail\",\n",
        "                \"judge_reasoning\": f\"{coverage_reason}; {tone_reason}\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    judged = df.merge(pd.DataFrame(rows), on=\"trace_id\")\n",
        "    judged[\"judge_name\"] = name\n",
        "    return judged\n",
        "\n",
        "\n",
        "BASELINE_CONFIG = {\n",
        "    \"coverage_column\": \"literal_coverage\",\n",
        "    \"coverage_threshold\": 0.70,\n",
        "    \"informal_threshold\": 0.95,\n",
        "    \"name\": \"Baseline few-shot set\",\n",
        "}\n",
        "\n",
        "IMPROVED_CONFIG = {\n",
        "    \"coverage_column\": \"semantic_coverage\",\n",
        "    \"coverage_threshold\": 0.95,\n",
        "    \"informal_threshold\": 0.30,\n",
        "    \"name\": \"Improved few-shot set\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_split(df: pd.DataFrame, config: Dict) -> Dict[str, float]:\n",
        "    judged = run_judge(df, **config)\n",
        "    labels = df[\"human_pass\"].astype(int)\n",
        "    preds = (judged[\"judge_answer\"] == \"Pass\").astype(int)\n",
        "\n",
        "    tp = int(((labels == 1) & (preds == 1)).sum())\n",
        "    tn = int(((labels == 0) & (preds == 0)).sum())\n",
        "    fp = int(((labels == 0) & (preds == 1)).sum())\n",
        "    fn = int(((labels == 1) & (preds == 0)).sum())\n",
        "\n",
        "    total = len(df)\n",
        "    accuracy = (tp + tn) / total if total else 0.0\n",
        "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "    tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
        "\n",
        "    return {\n",
        "        \"judge_name\": config[\"name\"],\n",
        "        \"TP\": tp,\n",
        "        \"FP\": fp,\n",
        "        \"TN\": tn,\n",
        "        \"FN\": fn,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"TPR\": tpr,\n",
        "        \"TNR\": tnr,\n",
        "    }\n",
        "\n",
        "\n",
        "def confusion_table(df: pd.DataFrame, config: Dict) -> pd.DataFrame:\n",
        "    judged = run_judge(df, **config)\n",
        "    ctab = pd.crosstab(df[\"human_pass\"], judged[\"judge_answer\"], rownames=[\"Human\"], colnames=[\"Judge\"], dropna=False)\n",
        "    return ctab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Baseline judge on dev split\n",
        "The baseline prompt under-penalises slang and misses paraphrased decisions. We see this in the confusion matrix and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_confusion = confusion_table(splits[\"dev\"], BASELINE_CONFIG)\n",
        "baseline_metrics = evaluate_split(splits[\"dev\"], BASELINE_CONFIG)\n",
        "baseline_confusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect a few disagreements to understand failure patterns (exact summaries truncated here for readability).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_judged_dev = run_judge(splits[\"dev\"], **BASELINE_CONFIG)\n",
        "dev_disagreements = baseline_judged_dev.assign(human_pass=splits[\"dev\"][\"human_pass\"])\n",
        "dev_disagreements = dev_disagreements[dev_disagreements[\"human_pass\"] != (dev_disagreements[\"judge_answer\"] == \"Pass\")]\n",
        "dev_disagreements[['trace_id', 'human_pass', 'judge_answer', 'judge_reasoning']].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After reviewing the disagreements we add sharper few-shot examples: one that demonstrates professional tone while paraphrasing a decision, and another that calls out slang like “super pumped.” That guides the judge toward semantic coverage and stricter tone enforcement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "improved_confusion = confusion_table(splits[\"dev\"], IMPROVED_CONFIG)\n",
        "improved_metrics = evaluate_split(splits[\"dev\"], IMPROVED_CONFIG)\n",
        "improved_confusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "improved_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparison = pd.DataFrame([baseline_metrics, improved_metrics]).set_index(\"judge_name\")[[\"Accuracy\", \"TPR\", \"TNR\"]]\n",
        "comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy jumps once the improved examples teach the judge to accept paraphrases (higher TPR) and reject informal tone (higher TNR).\n",
        "\n",
        "With the prompt frozen we move to the test split for an unbiased estimate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_confusion = confusion_table(splits[\"test\"], IMPROVED_CONFIG)\n",
        "test_metrics = evaluate_split(splits[\"test\"], IMPROVED_CONFIG)\n",
        "test_confusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correcting Success Rates\n",
        "An imperfect judge biases raw success rates. We use the test-set TPR/TNR to correct the observed pass rate on a mock production batch and quantify uncertainty with bootstrap sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rogan_gladen(p_obs: float, tpr: float, tnr: float) -> float:\n",
        "    denominator = tpr + tnr - 1\n",
        "    if denominator == 0:\n",
        "        return float(\"nan\")\n",
        "    corrected = (p_obs + tnr - 1) / denominator\n",
        "    return float(min(max(corrected, 0.0), 1.0))\n",
        "\n",
        "\n",
        "production_sample = labeled_summaries.sample(n=30, random_state=321).copy()\n",
        "production_results = run_judge(production_sample, **IMPROVED_CONFIG)\n",
        "\n",
        "observed_pass_rate = (production_results[\"judge_answer\"] == \"Pass\").mean()\n",
        "corrected_pass_rate = rogan_gladen(observed_pass_rate, test_metrics[\"TPR\"], test_metrics[\"TNR\"])\n",
        "\n",
        "{\"observed_pass_rate\": observed_pass_rate, \"corrected_pass_rate\": corrected_pass_rate}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e9ccec",
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_corrected_rate(test_df: pd.DataFrame, observed_rate: float, *, draws: int = 5000, seed: int = 99) -> Dict[str, float]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    labels = test_df[\"human_pass\"].astype(int).to_numpy()\n",
        "    preds = (run_judge(test_df, **IMPROVED_CONFIG)[\"judge_answer\"] == \"Pass\").astype(int).to_numpy()\n",
        "    n = len(test_df)\n",
        "\n",
        "    samples = []\n",
        "    for _ in range(draws):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        sampled_labels = labels[idx]\n",
        "        sampled_preds = preds[idx]\n",
        "\n",
        "        tp = ((sampled_labels == 1) & (sampled_preds == 1)).sum()\n",
        "        fn = ((sampled_labels == 1) & (sampled_preds == 0)).sum()\n",
        "        tn = ((sampled_labels == 0) & (sampled_preds == 0)).sum()\n",
        "        fp = ((sampled_labels == 0) & (sampled_preds == 1)).sum()\n",
        "\n",
        "        tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "        tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
        "        denominator = tpr + tnr - 1\n",
        "        if denominator == 0:\n",
        "            continue\n",
        "        corrected = (observed_rate + tnr - 1) / denominator\n",
        "        samples.append(min(max(corrected, 0.0), 1.0))\n",
        "\n",
        "    if not samples:\n",
        "        return {\"lower\": float(\"nan\"), \"upper\": float(\"nan\")}\n",
        "\n",
        "    lower, upper = np.percentile(samples, [2.5, 97.5])\n",
        "    return {\"lower\": float(lower), \"upper\": float(upper)}\n",
        "\n",
        "ci = bootstrap_corrected_rate(splits[\"test\"], observed_pass_rate, draws=3000)\n",
        "ci\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways & Next Steps\n",
        "- Start with programmatic checks for crisp, rule-based spec failures (tone, required boilerplate).\n",
        "- Use LLM-as-judge evaluators for nuanced behaviors like decision coverage; align them with few-shot iteration on a dev split, then freeze and score test.\n",
        "- Correct raw success rates with judge accuracy stats and attach confidence intervals before comparing prompt variants.\n",
        "- To plug this into the real workshop data, swap the synthetic dataset for traces from `data/email_annotations.duckdb`, move the judge stub into `tools/`, and call your LLM provider instead of the heuristic.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
