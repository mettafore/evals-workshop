{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19129233",
   "metadata": {},
   "source": [
    "# 01 · Email Eval Brief & Prompt Foundations\n",
    "\n",
    "This notebook introduces GenAI evaluation fundamentals using the curated Enron email slice from `data/curated_emails.csv`. The goals are to clarify why GenAI evals differ from classic ML metrics, align on the Analyze → Measure → Improve loop, and prepare rubric + prompt assets for the remainder of the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2363c",
   "metadata": {},
   "source": [
    "## What Are GenAI Evals?\n",
    "\n",
    "- **Definition**: GenAI evaluations systematically measure the quality of high-dimensional outputs (long-form text, multi-field JSON, images, etc.) produced by generative models such as large language models (LLMs). A good evaluation can be interpreted without ambiguity, is reproducible, and provides actionable insights to improve model performance.\n",
    "- **Contrast with classic ML**: Traditional ML metrics (accuracy, precision, recall) evaluate low-dimensional outputs (e.g., class labels, scalar predictions). GenAI outputs are complex and multifaceted, requiring more nuanced evaluation approaches.\n",
    "\n",
    "In GenAI applications, the evaluation and testing process is non-trivial and often requires human judgment. This is because generative models can produce a wide range of outputs, and the quality of these outputs can be subjective and context-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb401358",
   "metadata": {},
   "source": [
    "### Why Are GenAI Evals Hard?\n",
    "\n",
    "1. **Subjective, multifaceted outputs**: Multiple correct summaries/styles exist; we need rubrics to normalize judgments. There is no one correct answer to compare against.\n",
    "2. **Non-determinism**: Identical prompts can produce different answers, so we care about failure rates and distributions, not single runs.\n",
    "3. **Specification drift**: Requirements emerge as we inspect outputs, meaning evaluation criteria evolve alongside prompts and product goals.\n",
    "4. **Label scarcity**: Golden answers are expensive. Unlike traditional ML datasets, we often lack large-scale labeled data for training and evaluation. Reference-free rules and Subject Matter Expert (SME) participation become crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44532ea1",
   "metadata": {},
   "source": [
    "## The Three Gulfs<sup>1</sup>\n",
    "\n",
    "The 3 major challenges in building effective GenAI systems can be conceptualized as three \"gulfs\" that separate the developer intentions from the LLM pipeline and data (inputs and outputs).\n",
    "\n",
    "![Three Gulfs](3_gulfs.jpeg)\n",
    "\n",
    "- **Gulf of Comprehension**: Hard to understand pipeline behaviour at scale → we sample data, inspect outputs, and identify failure modes.\n",
    "- **Gulf of Specification**: What we mean ≠ what we specify → prompts/instructions must be explicit, data-informed.\n",
    "- **Gulf of Generalization**: Clear prompts still fail on new inputs → measurable evals help us detect and mitigate drift.\n",
    "\n",
    "We will repeatedly traverse **Analyze → Measure → Improve** to bridge these gulfs throughout the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f94da",
   "metadata": {},
   "source": [
    "## The Analyze - Measure - Improve Cycle\n",
    "\n",
    "In order to bridge the three gulfs, we will repeatedly traverse the Analyze → Measure → Improve cycle.\n",
    "\n",
    "![The Analyze - Measure - Improve Cycle](AMI-cycle.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2ed3c",
   "metadata": {},
   "source": [
    "## Load the Curated Dataset\n",
    "\n",
    "Notebook `00-Obtain-Candidate-Set.ipynb` produced `data/curated_emails.csv` via:\n",
    "- `TIME_WINDOW = (2001-03-01, 2001-06-30)`\n",
    "- `ACTION_KEYWORDS`, `MAX_RECIPIENTS = 6`\n",
    "- `BROADCAST_SUBJECT_KEYWORDS`, `MAX_SUBJECT_FREQUENCY = 50`\n",
    "- Quote-depth heuristics (`MIN_QUOTE_MARKERS = 2`) and `MAX_BODY_CHARS = 5000`\n",
    "- Hand curated 100 emails. \n",
    "\n",
    "Run the cell below to load the slice (or supply your own CSV via `CANDIDATE_PATH`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, display\n",
    "import html\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CANDIDATE_PATH = Path(\"../data/curated_emails.csv\")\n",
    "\n",
    "emails_df = pd.read_csv(CANDIDATE_PATH)\n",
    "print(f\"Loaded {len(emails_df):,} emails from {CANDIDATE_PATH}\")\n",
    "display(emails_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd8f97",
   "metadata": {},
   "source": [
    "### Interactive Email Explorer\n",
    "\n",
    "Use the widget below to browse individual emails. This makes it easier to inspect quoted history, commitments, and tone without scrolling through raw tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_count = len(emails_df)\n",
    "if email_count == 0:\n",
    "    raise ValueError(\"emails_df is empty; nothing to explore.\")\n",
    "\n",
    "_email_card_style_injected = False\n",
    "\n",
    "\n",
    "def _ensure_email_card_style() -> None:\n",
    "    \"\"\"Inject a lightweight CSS card style once per session.\"\"\"\n",
    "    global _email_card_style_injected\n",
    "    if _email_card_style_injected:\n",
    "        return\n",
    "    card_style = \"\"\"\n",
    "    <style>\n",
    "        .email-card {\n",
    "            font-family: var(--jp-content-font-family, 'Segoe UI', system-ui, sans-serif);\n",
    "            border: 1px solid rgba(15, 23, 42, 0.12);\n",
    "            border-radius: 12px;\n",
    "            padding: 0.9rem 1.1rem;\n",
    "            background: rgba(244, 246, 252, 0.9);\n",
    "            box-shadow: 0 4px 12px rgba(15, 23, 42, 0.06);\n",
    "            color: #1f2933;\n",
    "            max-width: 100%;\n",
    "        }\n",
    "        .email-card__header {\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            justify-content: space-between;\n",
    "            align-items: baseline;\n",
    "            gap: 0.75rem;\n",
    "            margin-bottom: 0.75rem;\n",
    "            font-weight: 600;\n",
    "        }\n",
    "        .email-card__header span:last-child {\n",
    "            font-size: 0.85rem;\n",
    "            font-weight: 500;\n",
    "            color: #52606d;\n",
    "        }\n",
    "        .email-card__meta {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));\n",
    "            gap: 0.45rem 1.2rem;\n",
    "            margin-bottom: 0.9rem;\n",
    "        }\n",
    "        .email-card__meta-label {\n",
    "            font-size: 0.75rem;\n",
    "            letter-spacing: 0.05em;\n",
    "            text-transform: uppercase;\n",
    "            color: #617180;\n",
    "            margin-bottom: 0.1rem;\n",
    "        }\n",
    "        .email-card__meta-value {\n",
    "            font-size: 0.95rem;\n",
    "            line-height: 1.35;\n",
    "            color: #1f2933;\n",
    "            word-break: break-word;\n",
    "        }\n",
    "        .email-card__body {\n",
    "            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;\n",
    "            background: rgba(255, 255, 255, 0.85);\n",
    "            border-radius: 10px;\n",
    "            padding: 0.9rem 1rem;\n",
    "            border: 1px solid rgba(15, 23, 42, 0.12);\n",
    "            white-space: pre-wrap;\n",
    "            overflow-y: auto;\n",
    "            max-height: 440px;\n",
    "        }\n",
    "        .email-card__body--truncated::after {\n",
    "            content: '\\u2026';\n",
    "            color: #8996a7;\n",
    "        }\n",
    "        .email-card__hint {\n",
    "            font-size: 0.8rem;\n",
    "            color: #55606f;\n",
    "            margin-top: 0.5rem;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(card_style))\n",
    "    _email_card_style_injected = True\n",
    "\n",
    "\n",
    "def _clean_subject(value: str) -> str:\n",
    "    if not value:\n",
    "        return \"(no subject)\"\n",
    "    text = str(value).strip().replace(\"\\n\", \" \")\n",
    "    return text if len(text) <= 80 else f\"{text[:77]}...\"\n",
    "\n",
    "\n",
    "def _format_email(idx: int, show_full_body: bool = False) -> widgets.HTML:\n",
    "    row = emails_df.iloc[idx]\n",
    "    meta_fields = [\n",
    "        (\"Subject\", row.get(\"subject\") or \"(no subject)\"),\n",
    "        (\"From\", row.get(\"from_email\") or row.get(\"from_raw\") or \"unknown\"),\n",
    "        (\"To\", row.get(\"to_emails\") or row.get(\"to_raw\") or \"—\"),\n",
    "        (\"CC\", row.get(\"cc_emails\") or row.get(\"cc_raw\") or \"—\"),\n",
    "        (\"Characters\", f\"{int(row.get('body_char_len', 0)):,}\"),\n",
    "        (\n",
    "            \"Quote markers\",\n",
    "            f\"{int(row.get('quote_separator_count', 0) + row.get('quote_line_count', 0)):,}\",\n",
    "        ),\n",
    "    ]\n",
    "    meta_html = \"\".join(\n",
    "        f\"<div><div class='email-card__meta-label'>{html.escape(label)}</div>\"\n",
    "        f\"<div class='email-card__meta-value'>{html.escape(str(value))}</div></div>\"\n",
    "        for label, value in meta_fields\n",
    "    )\n",
    "\n",
    "    body_raw = (row.get(\"body\") or \"\").strip()\n",
    "    truncated = not show_full_body and len(body_raw) > 1500\n",
    "    body_slice = body_raw if not truncated else body_raw[:1500].rstrip()\n",
    "    body_class = \"email-card__body\"\n",
    "    if truncated:\n",
    "        body_class += \" email-card__body--truncated\"\n",
    "    if body_slice:\n",
    "        body_html = html.escape(body_slice)\n",
    "        body_block = f\"<pre class='{body_class}'>{body_html}</pre>\"\n",
    "    else:\n",
    "        body_block = f\"<pre class='{body_class}'><em>empty body</em></pre>\"\n",
    "\n",
    "    hint = (\n",
    "        '<div class=\"email-card__hint\">Body truncated. Enable \"Show full body\" to view the remainder.</div>'\n",
    "        if truncated\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    card_html = f\"\"\"\n",
    "    <div class='email-card'>\n",
    "        <div class='email-card__header'>\n",
    "            <span>Email #{idx}</span>\n",
    "            <span>Row {idx + 1} of {email_count}</span>\n",
    "        </div>\n",
    "        <div class='email-card__meta'>{meta_html}</div>\n",
    "        {body_block}\n",
    "        {hint}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return widgets.HTML(card_html)\n",
    "\n",
    "\n",
    "slider = widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=email_count - 1,\n",
    "    step=1,\n",
    "    value=0,\n",
    "    description=\"Email index\",\n",
    "    continuous_update=False,\n",
    "    readout=False,\n",
    "    layout=widgets.Layout(width=\"340px\"),\n",
    ")\n",
    "show_full_body = widgets.Checkbox(\n",
    "    value=False, description=\"Show full body\", indent=False\n",
    ")\n",
    "\n",
    "subject_options = [\n",
    "    (f\"{idx:03} — {_clean_subject(emails_df.iloc[idx].get('subject'))}\", idx)\n",
    "    for idx in range(email_count)\n",
    "]\n",
    "jump_to = widgets.Dropdown(\n",
    "    options=subject_options,\n",
    "    value=0,\n",
    "    description=\"Jump to\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    ")\n",
    "\n",
    "header = widgets.HTML(\n",
    "    f\"<h3 style='margin-bottom:0.2rem'>Explore filtered emails ({email_count} total)</h3>\"\n",
    ")\n",
    "controls = widgets.VBox(\n",
    "    [widgets.HBox([slider, jump_to]), widgets.HBox([show_full_body])]\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(widgets.VBox([header, controls, output]))\n",
    "\n",
    "\n",
    "def _render(idx: int, reveal_full: bool) -> None:\n",
    "    _ensure_email_card_style()\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        display(_format_email(idx, reveal_full))\n",
    "\n",
    "\n",
    "def _sync_dropdown(idx: int) -> None:\n",
    "    if jump_to.value == idx:\n",
    "        return\n",
    "    jump_to.unobserve(_on_jump, names=\"value\")\n",
    "    jump_to.value = idx\n",
    "    jump_to.observe(_on_jump, names=\"value\")\n",
    "\n",
    "\n",
    "def _on_slider(change: dict) -> None:\n",
    "    if change[\"name\"] != \"value\" or change[\"new\"] is None:\n",
    "        return\n",
    "    _sync_dropdown(change[\"new\"])\n",
    "    _render(change[\"new\"], show_full_body.value)\n",
    "\n",
    "\n",
    "def _on_jump(change: dict) -> None:\n",
    "    if change[\"name\"] != \"value\" or change[\"new\"] is None:\n",
    "        return\n",
    "    if slider.value != change[\"new\"]:\n",
    "        slider.value = change[\"new\"]\n",
    "    else:\n",
    "        _render(change[\"new\"], show_full_body.value)\n",
    "\n",
    "\n",
    "def _on_show_full_body(change: dict) -> None:\n",
    "    if change[\"name\"] != \"value\":\n",
    "        return\n",
    "    _render(slider.value, change[\"new\"])\n",
    "\n",
    "\n",
    "slider.observe(_on_slider, names=\"value\")\n",
    "jump_to.observe(_on_jump, names=\"value\")\n",
    "show_full_body.observe(_on_show_full_body, names=\"value\")\n",
    "\n",
    "_render(slider.value, show_full_body.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142f0a4",
   "metadata": {},
   "source": [
    "## Our GenAI Problem: Email Summaries for Commitments\n",
    "\n",
    "We want concise summaries that capture:\n",
    "- Commitments made (tasks/action items)\n",
    "- Owners and due dates (if present)\n",
    "- Relevant summaries that get a new person up to speed or remind existing participants, while avoiding hallucinations.\n",
    "\n",
    "These long emails—with quoted history intact—provide enough context to practice failure discovery and eval design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d752b7f",
   "metadata": {},
   "source": [
    "## SME Alignment Models\n",
    "\n",
    "There are two common SME models for rubric definition and adjudication:\n",
    "- **Benevolent Dictator**: one lead defines rubric/labels, others review. Fast, consistent—ideal for tight timelines (our workshop default).\n",
    "- **Committee/Consensus**: multiple SMEs negotiate rubric and adjudicate disagreements. Useful for fuzzy domains, but slower.\n",
    "\n",
    "For this workshop we assume a *benevolent dictator* model to keep exercises crisp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becc4ae",
   "metadata": {},
   "source": [
    "## Rubric for Email Summaries\n",
    "\n",
    "### 1. Commitments Captured\n",
    "**Pass:** All explicit tasks/requests identified with enough detail to act on  \n",
    "**Fail:** Missing commitment OR hallucinated task not in original\n",
    "\n",
    "**Examples:**\n",
    "- *Pass*: \"Send Q3 report by Friday.\" → captured correctly\n",
    "- *Fail*: Commitment captured: \"Can someone review?\" → output assigns to John (hallucinated)\n",
    "\n",
    "### 2. Owner & Due Date Accuracy  \n",
    "**Pass:** Owners/dates match email exactly; if absent → \"Owner: Unknown\" or \"Due: Not stated\"  \n",
    "**Fail:** Invented owner/date OR misattributed responsibility\n",
    "\n",
    "**Examples:**\n",
    "- *Pass*: Email silent on owner → \"Owner: Unknown\"\n",
    "- *Fail*: Email says \"someone handle this\" → output says \"Owner: Sarah\"\n",
    "\n",
    "### 3. No Hallucinations in Summary\n",
    "**Pass:** Summary contains only facts from email; uses qualifiers for uncertainty  \n",
    "**Fail:** Invented details, fabricated numbers, OR missing critical context\n",
    "\n",
    "**Examples:**\n",
    "- *Pass*: Email mentions \"budget concerns\" → summary says \"budget concerns raised\"\n",
    "- *Fail*: Email mentions \"budget concerns\" → summary says \"budget cut by 15%\"\n",
    "\n",
    "### 4. Clarity & Brevity\n",
    "**Pass:** ≤120 words, scannable, actionable language  \n",
    "**Fail:** >120 words OR unclear/rambling\n",
    "\n",
    "**Examples:**\n",
    "- *Pass*: \"Team needs vendor approval by EOW for Q4 launch\"\n",
    "- *Fail*: \"The team was discussing and there are some things about vendors...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a9394",
   "metadata": {},
   "source": [
    "## Prompt Engineering Best Practices\n",
    "\n",
    "When drafting your first summary prompt:\n",
    "1. **Role & Objective** – e.g., “You are an operations analyst summarizing corporate email threads.”\n",
    "2. **Instructions** – enumerate required/forbidden behaviours (mention rubric criteria).\n",
    "3. **Context** – delimit the email body clearly.\n",
    "4. **Examples** – include one labeled summary if possible (even synthetic).\n",
    "5. **Reasoning steps** – ask the model to extract commitments before writing prose.\n",
    "6. **Output format** – enforce bullet lists or structured JSON if you need programmatic checks.\n",
    "7. **Safety clauses** – remind the model not to invent owners/dates.\n",
    "\n",
    "We will iterate on this prompt in Notebook 03 with automated evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3f194",
   "metadata": {},
   "source": [
    "### Example Prompt Template\n",
    "\n",
    "```plaintext\n",
    "# Startup Idea Generator\n",
    "\n",
    "## Role\n",
    "You are a startup advisor identifying viable business opportunities from customer problems.\n",
    "\n",
    "## Instructions\n",
    "✅ **Do:**\n",
    "- Ground ideas in real problems from the chat\n",
    "- Specify target customers and revenue model\n",
    "- Assess with existing technology only\n",
    "\n",
    "❌ **Don't:**\n",
    "- Suggest ideas needing non-existent tech\n",
    "- Ignore legal/ethical issues\n",
    "- Propose vague concepts like \"AI for everything\"\n",
    "\n",
    "## Context\n",
    "{chat_input}\n",
    "\n",
    "## Example\n",
    "**Input:** \"I waste 10 minutes daily finding matching Tupperware lids\"\n",
    "\n",
    "**Output:**\n",
    "- **Idea**: RFID kitchen containers that show lid matches via app\n",
    "- **Customer**: Busy families (12M households)  \n",
    "- **Revenue**: $89 starter kit + $5/mo subscription\n",
    "- **Differentiation**: Existing brands lack smart features\n",
    "\n",
    "## Process\n",
    "1. Extract core pain point\n",
    "2. Identify target customer\n",
    "3. Design feasible solution\n",
    "4. Define revenue model\n",
    "5. Check competition\n",
    "\n",
    "## Output Format\n",
    "- **Idea**: [One-liner]\n",
    "- **Problem**: [Pain point + scale]\n",
    "- **Solution**: [How it works]\n",
    "- **Customer**: [Who + market size]\n",
    "- **Revenue**: [Monetization]\n",
    "- **Competition**: [Alternatives + differentiation]\n",
    "\n",
    "## Safety\n",
    "- If vague → ask clarifying questions\n",
    "- If unclear → state \"Need more context\" \n",
    "- Don't invent numbers or assumptions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b71c7",
   "metadata": {},
   "source": [
    "### Starter Prompt Scaffold\n",
    "\n",
    "Fill in the template below during the live session or as homework. Adapt to your organizational style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "PROMPT_TEMPLATE = dedent(\"\"\"\n",
    "\n",
    "You are an operations analyst summarizing internal Enron email conversations and identifying key action items.\n",
    "\n",
    "Instructions:\n",
    "1. Identify every explicit commitment or request and list it as a bullet.\n",
    "2. For each commitment, state the responsible owner and due date if it exists; otherwise write `Owner: Unknown` or `Due: Not stated`.\n",
    "3. Do not invent facts that are not present in the email.\n",
    "\n",
    "Email metadata:\n",
    "Subject: {subject}\n",
    "From: {from_line}\n",
    "To: {to_line}\n",
    "Cc: {cc_line}\n",
    "\n",
    "Email body (delimited by triple backticks):\n",
    "```\n",
    "{body}\n",
    "```\n",
    "\n",
    "Return your answer as JSON with keys `summary` (string) and `commitments` (array of strings).\n",
    "\"\"\")\n",
    "\n",
    "print(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "**Reminder:** If you iterate on the template, save it under a new filename (e.g., `prompts/email_summary_prompt_v2.txt`) and pass that path via `--prompt` when you run `tools/generate_email_traces.py`. Each run records the prompt path and checksum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = \"v1\"\n",
    "PROMPT_PATH = Path(f\"../prompts/email_summary_prompt_{version}.txt\")\n",
    "PROMPT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "PROMPT_PATH.write_text(PROMPT_TEMPLATE.strip() + \"\\n\")\n",
    "print(f\"Saved prompt to {PROMPT_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b9cb9",
   "metadata": {},
   "source": [
    "### Prompt Output Example\n",
    "The trace generator (Notebook 02) expects a structured response matching this Pydantic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b190b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class SummaryPayload(BaseModel):\n",
    "    summary: str = Field(..., description=\"Concise email summary\")\n",
    "    commitments: List[str] = Field(\n",
    "        default_factory=list, description=\"Explicit commitments or action items\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856f43f",
   "metadata": {},
   "source": [
    "### Try the Prompt with Pydantic AI\n",
    "Run the cell below after setting an API key (e.g., `OPENAI_API_KEY`). It loads the saved system/user prompts, picks the first filtered email, and validates the response against `SummaryPayload`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.exceptions import UnexpectedModelBehavior\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "\n",
    "DATA_PATH = Path(\"../data/curated_emails.csv\")\n",
    "PROMPT_TEMPLATE_PATH = Path(\"../prompts/email_summary_prompt.txt\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Run earlier cells to generate ../data/curated_emails.csv first.\"\n",
    "    )\n",
    "\n",
    "prompt_template = PROMPT_TEMPLATE_PATH.read_text(encoding=\"utf-8\")\n",
    "emails_df = pd.read_csv(DATA_PATH)\n",
    "row = emails_df.iloc[0]\n",
    "\n",
    "\n",
    "def _fmt(value, default=\"Unknown\"):\n",
    "    if pd.isna(value):\n",
    "        return default\n",
    "    text = str(value).strip()\n",
    "    return text or default\n",
    "\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    subject=row.get(\"subject\") or \"No subject\",\n",
    "    from_line=_fmt(row.get(\"from_email\") or row.get(\"from_raw\")),\n",
    "    to_line=_fmt(\n",
    "        row.get(\"to_emails\") or row.get(\"to_raw\"),\n",
    "        default=\"(no direct recipients recorded)\",\n",
    "    ),\n",
    "    cc_line=_fmt(\n",
    "        row.get(\"cc_emails\") or row.get(\"cc_raw\"), default=\"(no cc recipients recorded)\"\n",
    "    ),\n",
    "    body=row.get(\"body\", \"\"),\n",
    ")\n",
    "\n",
    "model_name = os.environ.get(\"PYDANTIC_AI_MODEL\", \"openai:gpt-4o-mini\")\n",
    "agent = Agent(model_name, system_prompt=\"\")\n",
    "\n",
    "try:\n",
    "    # Use await instead of run_sync() to work with Jupyter's existing event loop\n",
    "    result = await agent.run(prompt, output_type=SummaryPayload)\n",
    "except UnexpectedModelBehavior as exc:\n",
    "    raise RuntimeError(\n",
    "        \"Model response did not match SummaryPayload. Review your prompt or model settings.\"\n",
    "    ) from exc\n",
    "\n",
    "pprint(\"EMAIL BODY\")\n",
    "pprint(row.get(\"body\", \"\"))\n",
    "pprint('\\n')\n",
    "pprint('\\n')\n",
    "pprint(\"SUMMARY\")\n",
    "pprint(result.output.summary)\n",
    "pprint(len(result.output.summary.split(\" \")))\n",
    "pprint('\\n')\n",
    "pprint('\\n')\n",
    "pprint(\"COMMITMENTS\")\n",
    "pprint(result.output.commitments)   \n",
    "result.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.output.commitments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.output.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec947ae",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "1. Shankar, Shreya, et al. \"Steering semantic data processing with docwrangler.\" Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology. 2025.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02359f",
   "metadata": {},
   "source": [
    "## What’s Next\n",
    "\n",
    "- Notebook 02: convert manual notes into open/axial codes (failure taxonomy).\n",
    "- Notebook 03: turn rubric items into automated evaluators (LLM-as-a-judge + checks).\n",
    "- Optional Homework 01a: generate synthetic emails to stress-test new failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3574e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evals-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
