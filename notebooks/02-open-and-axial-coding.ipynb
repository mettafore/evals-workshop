{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Homework 02: Open & Axial Coding Walkthrough\n",
    "\n",
    "Grounded-theory error analysis for the email summariser workshop. Use this notebook to link qualitative insights from the annotation tool back into the Analyze → Measure → Improve loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereqs",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "- Run Notebook 00/01a to prepare either the filtered or synthetic email CSV.\n",
    "- Use `tools/generate_email_traces.py` to create traces for the current Git commit (short SHA stored as the `run_id`). The optional cell below lets you run it in-place during a workshop demo.\n",
    "- Launch `tools/email_annotation_app.py` to collect open coding notes and failure modes in the browser, then return here to analyse the DuckDB tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "TRACE_ROOT = Path(\"../annotation/traces\")\n",
    "DUCKDB_PATH = DATA_DIR / \"email_annotations.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datasource",
   "metadata": {},
   "source": [
    "### 1. Choose Attributes for Generating Traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PROMPT_PATH = Path(\"../prompts/email_summary_prompt.txt\")\n",
    "DEFAULT_MODEL = os.environ.get(\"PYDANTIC_AI_MODEL\")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"GPT-5 mini\", \"openai:gpt-5-mini\"),\n",
    "        (\"GPT-5\", \"openai:gpt-5\"),\n",
    "        (\"Claude 4.5 Sonnet\", \"anthropic:claude-4-5-sonnet\"),\n",
    "    ],\n",
    "    value=DEFAULT_MODEL,\n",
    "    description=\"GenAI Model:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "prompt_path_widget = widgets.Text(\n",
    "    value=str(DEFAULT_PROMPT_PATH.resolve()),\n",
    "    description=\"Prompt Template:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "workers_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description=\"LLM Workers:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "limit_slider = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description=\"Email Limit:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "display(model_dropdown, prompt_path_widget, workers_slider, limit_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE_PATH = DATA_DIR / \"curated_emails.csv\"\n",
    "Markdown(f\"**Using:** `{DATA_SOURCE_PATH}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trace-gen",
   "metadata": {},
   "source": [
    "### 2. (Optional) Regenerate traces for this run\n",
    "Set the flag to `True` to call `tools/generate_email_traces.py` using the selected dataset. The script writes trace JSON under `annotation/traces/<run_id>` and upserts rows into DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-trace-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRACE_GENERATOR = True  # toggle to True for live demos\n",
    "\n",
    "if RUN_TRACE_GENERATOR:\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(Path(\"../tools/generate_email_traces.py\")),\n",
    "        \"--emails\",\n",
    "        str(DATA_SOURCE_PATH.resolve()),\n",
    "        \"--out\",\n",
    "        str(TRACE_ROOT.resolve()),\n",
    "        \"--model\",\n",
    "        model_dropdown.value,\n",
    "        \"--prompt\",\n",
    "        str(Path(prompt_path_widget.value).resolve()),\n",
    "        \"--workers\",\n",
    "        str(workers_slider.value),\n",
    "        \"--limit\",\n",
    "        str(limit_slider.value),\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-runs",
   "metadata": {},
   "source": [
    "### 3. Verify available trace runs\n",
    "Each trace directory name equals the short Git SHA captured when the generator script ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-active-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_runs = []\n",
    "if TRACE_ROOT.exists():\n",
    "    available_runs = sorted(p.name for p in TRACE_ROOT.iterdir() if p.is_dir())\n",
    "\n",
    "if not available_runs:\n",
    "    raise RuntimeError(\n",
    "        \"No trace runs detected under ../annotation/traces. Generate traces before proceeding.\"\n",
    "    )\n",
    "\n",
    "# Connect to DB to get the latest run by generated_at timestamp\n",
    "temp_conn = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "latest_run = temp_conn.execute(\"SELECT run_id FROM trace_runs ORDER BY generated_at DESC LIMIT 1\").fetchone()\n",
    "temp_conn.close()\n",
    "\n",
    "if latest_run:\n",
    "    ACTIVE_RUN_ID = latest_run[0]\n",
    "else:\n",
    "    # Fallback to alphabetically last directory if no runs in DB\n",
    "    ACTIVE_RUN_ID = available_runs[-1]\n",
    "\n",
    "Markdown(f\"**Active run:** `{ACTIVE_RUN_ID}` (latest by timestamp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annotation-tool",
   "metadata": {},
   "source": [
    "### Annotation Tool (Browser UI)\n",
    "- **Launch:** `uv run python tools/email_annotation_app.py` from the repo root.\n",
    "- **Navigate:** open `http://localhost:5001`.\n",
    "- **Controls:** \n",
    "  - `Z` – mark pass (instant judgment)\n",
    "  - `X` – mark fail (instant judgment)\n",
    "  - `Esc` – remove judgment (instant delete)\n",
    "  - `N` – add/edit note (observation)\n",
    "  - `F` – link failure modes (fail only)\n",
    "  - `←/→` – navigate between emails\n",
    "- **Auto-save:** every change is written directly to DuckDB.\n",
    "- Capture judgments, notes, and failure modes before returning to this notebook to analyze results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-db",
   "metadata": {},
   "source": [
    "### 4. Connect to DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db-connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect(str(DUCKDB_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-exploration",
   "metadata": {},
   "source": [
    "## Schema Exploration\n",
    "Explore each table in the DuckDB annotation database to understand the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labelers-header",
   "metadata": {},
   "source": [
    "### 1. `labelers` table\n",
    "**Purpose**: Tracks who is annotating emails in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labelers-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelers_df = conn.execute(\"SELECT * FROM labelers ORDER BY created_at\").df()\n",
    "display(Markdown(f\"**Columns**: `{', '.join(labelers_df.columns.tolist())}`\"))\n",
    "display(labelers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "runs-header",
   "metadata": {},
   "source": [
    "### 2. `trace_runs` table\n",
    "**Purpose**: Tracks each trace generation run (identified by Git commit SHA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "runs-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = conn.execute(\"SELECT * FROM trace_runs ORDER BY generated_at DESC\").df()\n",
    "display(Markdown(f\"**Columns**: `{', '.join(runs_df.columns.tolist())}`\"))\n",
    "display(runs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judgments-header",
   "metadata": {},
   "source": [
    "### 3. `email_judgments` table\n",
    "**Purpose**: Stores pass/fail decisions for each email. One row per email/labeler/run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVE_RUN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judgments-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments_df = conn.execute(\n",
    "    \"SELECT * FROM email_judgments WHERE run_id = ? ORDER BY judged_at DESC LIMIT 10\",\n",
    "    (ACTIVE_RUN_ID,)\n",
    ").df()\n",
    "display(Markdown(f\"**Columns**: `{', '.join(judgments_df.columns.tolist())}`\"))\n",
    "display(judgments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annotations-header",
   "metadata": {},
   "source": [
    "### 4. `annotations` table\n",
    "**Purpose**: Stores optional observation notes for emails (one note per email/labeler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annotations-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = conn.execute(\n",
    "    \"SELECT * FROM annotations WHERE run_id = ? ORDER BY created_at DESC LIMIT 10\",\n",
    "    (ACTIVE_RUN_ID,)\n",
    ").df()\n",
    "display(Markdown(f\"**Columns**: `{', '.join(annotations_df.columns.tolist())}`\"))\n",
    "display(annotations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failure-modes-header",
   "metadata": {},
   "source": [
    "### 5. `failure_modes` table\n",
    "**Purpose**: Catalog of failure mode definitions used for axial coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failure-modes-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_modes_df = conn.execute(\"SELECT * FROM failure_modes ORDER BY display_name\").df()\n",
    "display(Markdown(f\"**Columns**: `{', '.join(failure_modes_df.columns.tolist())}`\"))\n",
    "display(failure_modes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "axial-links-header",
   "metadata": {},
   "source": [
    "### 6. `axial_links` table\n",
    "**Purpose**: Links annotations to failure modes (many-to-many relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axial-links-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_links_df = conn.execute(\n",
    "    \"SELECT * FROM axial_links WHERE run_id = ? ORDER BY linked_at DESC LIMIT 10\",\n",
    "    (ACTIVE_RUN_ID,)\n",
    ").df()\n",
    "display(Markdown(f\"**Columns**: `{', '.join(axial_links_df.columns.tolist())}`\"))\n",
    "display(axial_links_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "master-table-header",
   "metadata": {},
   "source": [
    "## Denormalized Master Table\n",
    "Create a comprehensive view joining: email content + metadata + judgments + notes + failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-emails",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails_from_traces(run_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load all emails for a run from trace JSON files.\"\"\"\n",
    "    run_dir = TRACE_ROOT / run_id\n",
    "    if not run_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    emails = []\n",
    "    for trace_file in sorted(run_dir.glob(\"trace_*.json\")):\n",
    "        email_hash = trace_file.stem.replace(\"trace_\", \"\")\n",
    "        try:\n",
    "            trace_data = json.loads(trace_file.read_text(encoding=\"utf-8\"))\n",
    "            metadata = trace_data.get(\"metadata\", {}).get(\"extra\", {})\n",
    "            \n",
    "            # Extract subject from metadata\n",
    "            subject = metadata.get(\"normalized_subject\", \"\").title() or \"(no subject)\"\n",
    "            \n",
    "            # Extract body from request content\n",
    "            request_content = trace_data.get(\"request\", {}).get(\"messages\", [{}])[0].get(\"content\", \"\")\n",
    "            body_match = request_content.split(\"```\")\n",
    "            body = body_match[1].strip() if len(body_match) > 2 else \"\"\n",
    "            \n",
    "            emails.append({\n",
    "                \"email_hash\": email_hash,\n",
    "                \"subject\": subject,\n",
    "                \"body\": body,\n",
    "                \"Intent\": metadata.get(\"Intent\", None),\n",
    "                \"Designation\": metadata.get(\"Designation\", None),\n",
    "                \"Tone\": metadata.get(\"Tone\", None),\n",
    "                \"Context\": metadata.get(\"Context\", None),\n",
    "                \"run_id\": run_id,\n",
    "            })\n",
    "        except (json.JSONDecodeError, IOError):\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(emails)\n",
    "\n",
    "# Load all emails from trace files\n",
    "master_df = load_emails_from_traces(ACTIVE_RUN_ID)\n",
    "display(Markdown(f\"Loaded **{len(master_df):,}** emails from trace files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-judgments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all judgments for this run\n",
    "all_judgments = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT email_hash, labeler_id, pass_fail, judged_at\n",
    "    FROM email_judgments\n",
    "    WHERE run_id = ?\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,)\n",
    ").df()\n",
    "\n",
    "# Get all annotations for this run\n",
    "all_annotations = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT email_hash, labeler_id, open_code\n",
    "    FROM annotations\n",
    "    WHERE run_id = ?\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,)\n",
    ").df()\n",
    "\n",
    "# Get all failure modes linked to emails in this run\n",
    "failure_mode_links = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        a.email_hash,\n",
    "        fm.display_name,\n",
    "        fm.definition\n",
    "    FROM axial_links al\n",
    "    JOIN annotations a ON al.annotation_id = a.annotation_id\n",
    "    JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
    "    WHERE al.run_id = ?\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,)\n",
    ").df()\n",
    "\n",
    "# Aggregate failure modes per email (comma-separated)\n",
    "if not failure_mode_links.empty:\n",
    "    failure_agg = failure_mode_links.groupby(\"email_hash\").agg({\n",
    "        \"display_name\": lambda x: \", \".join(sorted(set(x))),\n",
    "        \"definition\": lambda x: \" | \".join(sorted(set(x)))\n",
    "    }).reset_index()\n",
    "    failure_agg.columns = [\"email_hash\", \"failure_modes\", \"failure_mode_definitions\"]\n",
    "else:\n",
    "    failure_agg = pd.DataFrame(columns=[\"email_hash\", \"failure_modes\", \"failure_mode_definitions\"])\n",
    "\n",
    "display(Markdown(f\"Found **{len(all_judgments):,}** judgments, **{len(all_annotations):,}** notes, **{len(failure_mode_links):,}** failure mode links\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join everything together (left join so unjudged emails show NULL)\n",
    "master_df = master_df.merge(\n",
    "    all_judgments[[\"email_hash\", \"pass_fail\", \"judged_at\", \"labeler_id\"]], \n",
    "    on=\"email_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "master_df = master_df.merge(\n",
    "    all_annotations[[\"email_hash\", \"open_code\"]], \n",
    "    on=\"email_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "master_df = master_df.merge(\n",
    "    failure_agg, \n",
    "    on=\"email_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Reorder columns for readability (only include columns that exist)\n",
    "base_columns = [\"email_hash\", \"subject\", \"body\"]\n",
    "metadata_columns = [\"Intent\", \"Designation\", \"Tone\", \"Context\"]\n",
    "judgment_columns = [\"pass_fail\", \"judged_at\", \"labeler_id\", \"open_code\", \"failure_modes\", \"failure_mode_definitions\", \"run_id\"]\n",
    "\n",
    "# Build column order: base + existing metadata + judgments\n",
    "column_order = base_columns + [col for col in metadata_columns if col in master_df.columns] + judgment_columns\n",
    "column_order = [col for col in column_order if col in master_df.columns]\n",
    "\n",
    "master_df = master_df[column_order]\n",
    "\n",
    "display(Markdown(f\"### Master Table: {len(master_df):,} rows\"))\n",
    "display(Markdown(\"One row per email. Unjudged emails show NULL for judgment columns.\"))\n",
    "master_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7674fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df[~master_df['failure_modes'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## Export Master Table\n",
    "Export the denormalized table as CSV for review in Excel/Google Sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT = True  # Toggle to False to skip export\n",
    "\n",
    "if EXPORT:\n",
    "    export_path = DATA_DIR / f\"email_analysis_{ACTIVE_RUN_ID}.csv\"\n",
    "    master_df.to_csv(export_path, index=False)\n",
    "    display(Markdown(f\"✅ **Exported** → `{export_path.name}`\"))\n",
    "    display(Markdown(f\"- **Total emails**: {len(master_df):,}\"))\n",
    "    display(Markdown(f\"- **Judged**: {master_df['pass_fail'].notna().sum():,}\"))\n",
    "    display(Markdown(f\"- **Pass**: {(master_df['pass_fail'] == True).sum():,}\"))\n",
    "    display(Markdown(f\"- **Fail**: {(master_df['pass_fail'] == False).sum():,}\"))\n",
    "    display(Markdown(f\"- **Unjudged**: {master_df['pass_fail'].isna().sum():,}\"))\n",
    "else:\n",
    "    display(Markdown(\"⚠️ Export disabled. Set `EXPORT = True` to write CSV.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9236a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
