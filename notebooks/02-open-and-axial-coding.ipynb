{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Homework 02: Open & Axial Coding Walkthrough\n",
    "\n",
    "Grounded-theory error analysis for the email summariser workshop. Use this notebook to link qualitative insights from the annotation tool back into the Analyze → Measure → Improve loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "- Run Notebook 00/01a to prepare either the filtered or synthetic email CSV.\n",
    "- Use `tools/generate_email_traces.py` to create traces for the current Git commit (short SHA stored as the `run_id`). The optional cell below lets you run it in-place during a workshop demo.\n",
    "- Launch `tools/email_annotation_app.py` to collect open coding notes and failure modes in the browser, then return here to analyse the DuckDB tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "TRACE_ROOT = Path(\"../annotation/traces\")\n",
    "DUCKDB_PATH = DATA_DIR / \"email_annotations.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "### 1. Choose data source\n",
    "Pick between the filtered production slice and the synthetic seed set from Notebook 01a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OPTIONS = {\n",
    "    \"filtered\": DATA_DIR / \"filtered_emails.csv\",\n",
    "    \"synthetic\": DATA_DIR / \"synthetic_emails.csv\",\n",
    "}\n",
    "\n",
    "source_toggle = widgets.ToggleButtons(\n",
    "    options=[(\"Filtered Emails\", \"filtered\"), (\"Synthetic Seed Set\", \"synthetic\")],\n",
    "    value=\"filtered\" if (DATA_DIR / \"filtered_emails.csv\").exists() else \"synthetic\",\n",
    "    description=\"Dataset:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "display(source_toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_KEY = source_toggle.value\n",
    "DATA_SOURCE_PATH = DATA_OPTIONS[SELECTED_KEY]\n",
    "if not DATA_SOURCE_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected dataset at {DATA_SOURCE_PATH}. Run Notebook 00 or 01a first.\"\n",
    "    )\n",
    "\n",
    "Markdown(f\"**Using:** `{DATA_SOURCE_PATH}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "### 2. (Optional) Regenerate traces for this run\n",
    "Set the flag to `True` to call `tools/generate_email_traces.py` using the selected dataset. The script writes trace JSON under `annotation/traces/<run_id>` and upserts rows into DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRACE_GENERATOR = False  # toggle to True for live demos\n",
    "\n",
    "if RUN_TRACE_GENERATOR:\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(Path(\"../tools/generate_email_traces.py\")),\n",
    "        \"--emails\",\n",
    "        str(DATA_SOURCE_PATH.resolve()),\n",
    "        \"--out\",\n",
    "        str(TRACE_ROOT.resolve()),\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "### 3. Verify available trace runs\n",
    "Each trace directory name equals the short Git SHA captured when the generator script ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_runs = []\n",
    "if TRACE_ROOT.exists():\n",
    "    available_runs = sorted(p.name for p in TRACE_ROOT.iterdir() if p.is_dir())\n",
    "\n",
    "if not available_runs:\n",
    "    raise RuntimeError(\n",
    "        \"No trace runs detected under ../annotation/traces. Generate traces before proceeding.\"\n",
    "    )\n",
    "\n",
    "ACTIVE_RUN_ID = available_runs[-1]\n",
    "Markdown(f\"**Active run:** `{ACTIVE_RUN_ID}` (showing most recent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "### 4. Connect to DuckDB and inspect trace runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect(str(DUCKDB_PATH))\n",
    "\n",
    "runs_df = conn.execute(\n",
    "    \"SELECT run_id, prompt_path, source_csv, generated_at FROM trace_runs ORDER BY generated_at DESC\"\n",
    ").df()\n",
    "\n",
    "display(runs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "source": [
    "### 5. Preview the chosen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv(DATA_SOURCE_PATH)\n",
    "Markdown(f\"Loaded **{len(emails_df):,}** emails from `{DATA_SOURCE_PATH.name}`\")\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "source": [
    "### 6. Emails registered for the active run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_raw_df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT email_id, subject, metadata, run_id, ingested_at\n",
    "    FROM emails_raw\n",
    "    WHERE run_id = ?\n",
    "    ORDER BY email_id\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,),\n",
    ").df()\n",
    "\n",
    "if emails_raw_df.empty:\n",
    "    raise RuntimeError(\n",
    "        f\"No emails found in DuckDB for run_id={ACTIVE_RUN_ID}. Re-run the trace generator.\"\n",
    "    )\n",
    "\n",
    "emails_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "source": [
    "### 7. Open coding inventory\n",
    "Pull annotations captured through the web tool (auto-saved to DuckDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT email_id, annotation_id, labeler_id, open_code, pass_fail, created_at\n",
    "    FROM annotations\n",
    "    WHERE run_id = ?\n",
    "    ORDER BY created_at\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,),\n",
    ").df()\n",
    "\n",
    "Markdown(f\"Collected **{len(annotations_df):,}** annotations for run `{ACTIVE_RUN_ID}`\")\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "source": [
    "### 8. Pass/Fail mix and annotation velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if annotations_df.empty:\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"⚠️ No annotations logged yet—open the web tool and capture a few observations.\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    annotations_df[\"status\"] = annotations_df[\"pass_fail\"].map(\n",
    "        {True: \"Fail\", False: \"Pass\", None: \"Unknown\"}\n",
    "    )\n",
    "    summary = annotations_df.groupby(\"status\").size().reset_index(name=\"count\")\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "source": [
    "### 9. Join annotations with email metadata\n",
    "Use JSON metadata stored in `emails_raw` to segment open codes by designation, tone, intent, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not annotations_df.empty:\n",
    "    metadata_records = emails_raw_df.set_index(\"email_id\")[\"metadata\"].to_dict()\n",
    "\n",
    "    def extract_field(email_id: str, field: str):\n",
    "        raw = metadata_records.get(email_id)\n",
    "        if not raw:\n",
    "            return None\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "        value = data.get(field)\n",
    "        if isinstance(value, (list, tuple)):\n",
    "            return \", \".join(str(v) for v in value)\n",
    "        return value\n",
    "\n",
    "    annotations_df[\"Intent\"] = annotations_df[\"email_id\"].apply(\n",
    "        lambda eid: extract_field(eid, \"Intent\")\n",
    "    )\n",
    "    annotations_df[\"Designation\"] = annotations_df[\"email_id\"].apply(\n",
    "        lambda eid: extract_field(eid, \"Designation\")\n",
    "    )\n",
    "    pivot = pd.crosstab(annotations_df[\"Intent\"], annotations_df[\"status\"])\n",
    "    display(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "source": [
    "### 10. Axial coding snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b79bc585a40fcaf58bf750017e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_modes_df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT fm.display_name, count(*) AS occurrences\n",
    "    FROM axial_links al\n",
    "    JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
    "    WHERE al.run_id = ?\n",
    "    GROUP BY 1\n",
    "    ORDER BY occurrences DESC\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,),\n",
    ").df()\n",
    "\n",
    "if failure_modes_df.empty:\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"⚠️ No failure modes linked yet—use the web tool (F) to start axial coding.\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    display(failure_modes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916684f9a58a4a2aa5f864670399430d",
   "metadata": {},
   "source": [
    "### 11. Failure-mode × Intent co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671c31a24314836a5b85d7ef7fbf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failure_modes_df.empty and not annotations_df.empty:\n",
    "    axial_df = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT al.annotation_id, al.failure_mode_id, fm.display_name, a.email_id\n",
    "        FROM axial_links al\n",
    "        JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
    "        JOIN annotations a ON al.annotation_id = a.annotation_id\n",
    "        WHERE al.run_id = ?\n",
    "        \"\"\",\n",
    "        (ACTIVE_RUN_ID,),\n",
    "    ).df()\n",
    "\n",
    "    if not axial_df.empty:\n",
    "        merged = axial_df.merge(\n",
    "            annotations_df[[\"annotation_id\", \"Intent\"]], on=\"annotation_id\", how=\"left\"\n",
    "        )\n",
    "        co_matrix = pd.crosstab(merged[\"display_name\"], merged[\"Intent\"])\n",
    "        display(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0902fd34d4ace834912fa1002cf8e",
   "metadata": {},
   "source": [
    "### 12. Export for sharing\n",
    "Write flattened tables so facilitators/stakeholders can review outside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa52606d8c4a75a9b52967216f8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT = False  # toggle to True to write CSV/JSON snapshots\n",
    "\n",
    "if EXPORT:\n",
    "    export_annotations = DATA_DIR / f\"email_annotations_{ACTIVE_RUN_ID}.csv\"\n",
    "    export_failure_modes = DATA_DIR / f\"failure_modes_{ACTIVE_RUN_ID}.csv\"\n",
    "    annotations_df.to_csv(export_annotations, index=False)\n",
    "    failure_modes_df.to_csv(export_failure_modes, index=False)\n",
    "    Markdown(\n",
    "        f\"Exported annotations → `{export_annotations.name}` and failure modes → `{export_failure_modes.name}`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1fa73e5044315a093ec459c9be902",
   "metadata": {},
   "source": [
    "---\n",
    "**Next steps:**\n",
    "1. Ensure each major failure mode has ≥20 failing examples before moving to Notebook 03.\n",
    "2. Re-run the prompt (Notebook 01) + trace generator after prompt fixes so you can compare runs (`run_id` == short Git SHA).\n",
    "3. Log qualitative takeaways and TODOs in the facilitation tracker / plan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}