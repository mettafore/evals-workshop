{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Homework 02: Open & Axial Coding Walkthrough\n",
    "\n",
    "Grounded-theory error analysis for the email summariser workshop. Use this notebook to link qualitative insights from the annotation tool back into the Analyze → Measure → Improve loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "- Run Notebook 00/01a to prepare either the filtered or synthetic email CSV.\n",
    "- Use `tools/generate_email_traces.py` to create traces for the current Git commit (short SHA stored as the `run_id`). The optional cell below lets you run it in-place during a workshop demo.\n",
    "- Launch `tools/email_annotation_app.py` to collect open coding notes and failure modes in the browser, then return here to analyse the DuckDB tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "TRACE_ROOT = Path(\"../annotation/traces\")\n",
    "DUCKDB_PATH = DATA_DIR / \"email_annotations.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "### 1. Choose data source\n",
    "Pick between the filtered production slice and the synthetic seed set from Notebook 01a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1f4d9715f0409d9e0ee0daa90acb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Dataset:', options=(('Filtered Emails', 'filtered'), ('Synthetic Seed Set', 'synthe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_OPTIONS = {\n",
    "    \"filtered\": DATA_DIR / \"filtered_emails_sample.csv\",\n",
    "    \"synthetic\": DATA_DIR / \"synthetic_emails.csv\",\n",
    "}\n",
    "\n",
    "source_toggle = widgets.ToggleButtons(\n",
    "    options=[(\"Filtered Emails\", \"filtered\"), (\"Synthetic Seed Set\", \"synthetic\")],\n",
    "    value=\"filtered\" if (DATA_DIR / \"filtered_emails.csv\").exists() else \"synthetic\",\n",
    "    description=\"Dataset:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "display(source_toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Using:** `../data/filtered_emails_sample.csv`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECTED_KEY = source_toggle.value\n",
    "DATA_SOURCE_PATH = DATA_OPTIONS[SELECTED_KEY]\n",
    "if not DATA_SOURCE_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected dataset at {DATA_SOURCE_PATH}. Run Notebook 00 or 01a first.\"\n",
    "    )\n",
    "\n",
    "Markdown(f\"**Using:** `{DATA_SOURCE_PATH}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "### 2. (Optional) Regenerate traces for this run\n",
    "Set the flag to `True` to call `tools/generate_email_traces.py` using the selected dataset. The script writes trace JSON under `annotation/traces/<run_id>` and upserts rows into DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/luvsuneja/Documents/repos/evals-workshop/.venv/bin/python ../tools/generate_email_traces.py --emails /Users/luvsuneja/Documents/repos/evals-workshop/data/filtered_emails_sample.csv --out /Users/luvsuneja/Documents/repos/evals-workshop/annotation/traces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working tree has uncommitted changes. Commit or stash before generating traces.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/Users/luvsuneja/Documents/repos/evals-workshop/.venv/bin/python', '../tools/generate_email_traces.py', '--emails', '/Users/luvsuneja/Documents/repos/evals-workshop/data/filtered_emails_sample.csv', '--out', '/Users/luvsuneja/Documents/repos/evals-workshop/annotation/traces']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      7\u001b[39m cmd = [\n\u001b[32m      8\u001b[39m     sys.executable,\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mstr\u001b[39m(Path(\u001b[33m\"\u001b[39m\u001b[33m../tools/generate_email_traces.py\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     \u001b[38;5;28mstr\u001b[39m(TRACE_ROOT.resolve()),\n\u001b[32m     14\u001b[39m ]\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(cmd))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-x86_64-none/lib/python3.12/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['/Users/luvsuneja/Documents/repos/evals-workshop/.venv/bin/python', '../tools/generate_email_traces.py', '--emails', '/Users/luvsuneja/Documents/repos/evals-workshop/data/filtered_emails_sample.csv', '--out', '/Users/luvsuneja/Documents/repos/evals-workshop/annotation/traces']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "RUN_TRACE_GENERATOR = True  # toggle to True for live demos\n",
    "\n",
    "if RUN_TRACE_GENERATOR:\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(Path(\"../tools/generate_email_traces.py\")),\n",
    "        \"--emails\",\n",
    "        str(DATA_SOURCE_PATH.resolve()),\n",
    "        \"--out\",\n",
    "        str(TRACE_ROOT.resolve()),\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "### 3. Verify available trace runs\n",
    "Each trace directory name equals the short Git SHA captured when the generator script ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_runs = []\n",
    "if TRACE_ROOT.exists():\n",
    "    available_runs = sorted(p.name for p in TRACE_ROOT.iterdir() if p.is_dir())\n",
    "\n",
    "if not available_runs:\n",
    "    raise RuntimeError(\n",
    "        \"No trace runs detected under ../annotation/traces. Generate traces before proceeding.\"\n",
    "    )\n",
    "\n",
    "ACTIVE_RUN_ID = available_runs[-1]\n",
    "Markdown(f\"**Active run:** `{ACTIVE_RUN_ID}` (showing most recent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64284c65",
   "metadata": {},
   "source": [
    "\n",
    "### Annotation Tool (Browser UI)\n",
    "- **Launch:** `python tools/email_annotation_app.py` from the repo root.\n",
    "- **Navigate:** open `http://localhost:5000`.\n",
    "- **Controls:** `A` adds an annotation (Enter saves, Esc cancels), `Z` marks **pass**, `X` marks **fail**, `F` links failure modes, `←/→` navigation.\n",
    "- **Auto-save:** every change is written directly to DuckDB.\n",
    "- Capture notes/failure modes before returning to this notebook to analyze results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "### 4. Connect to DuckDB and inspect trace runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect(str(DUCKDB_PATH))\n",
    "\n",
    "runs_df = conn.execute(\n",
    "    \"SELECT run_id, prompt_path, source_csv, generated_at FROM trace_runs ORDER BY generated_at DESC\"\n",
    ").df()\n",
    "\n",
    "display(runs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "source": [
    "### 5. Preview the chosen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv(DATA_SOURCE_PATH)\n",
    "Markdown(f\"Loaded **{len(emails_df):,}** emails from `{DATA_SOURCE_PATH.name}`\")\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "source": [
    "### 6. Emails registered for the active run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_raw_df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT email_hash, subject, metadata, run_id, ingested_at\n",
    "    FROM emails_raw\n",
    "    WHERE run_id = ?\n",
    "    ORDER BY email_hash\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,),\n",
    ").df()\n",
    "\n",
    "if emails_raw_df.empty:\n",
    "    raise RuntimeError(\n",
    "        f\"No emails found in DuckDB for run_id={ACTIVE_RUN_ID}. Re-run the trace generator.\"\n",
    "    )\n",
    "\n",
    "emails_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "source": [
    "### 7. Open coding inventory\n",
    "Pull annotations captured through the web tool (auto-saved to DuckDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT email_hash, annotation_id, labeler_id, open_code, pass_fail, created_at\n",
    "    FROM annotations\n",
    "    WHERE run_id = ?\n",
    "    ORDER BY created_at\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,),\n",
    ").df()\n",
    "\n",
    "Markdown(f\"Collected **{len(annotations_df):,}** annotations for run `{ACTIVE_RUN_ID}`\")\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "source": [
    "### 8. Pass/Fail mix and annotation velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if annotations_df.empty:\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"⚠️ No annotations logged yet—open the web tool and capture a few observations.\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    annotations_df[\"status\"] = annotations_df[\"pass_fail\"].map(\n",
    "        {True: \"Pass\", False: \"Fail\", None: \"Unknown\"}\n",
    "    )\n",
    "    summary = annotations_df.groupby(\"status\").size().reset_index(name=\"count\")\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "source": [
    "### 9. Join annotations with email metadata\n",
    "Use JSON metadata stored in `emails_raw` to segment open codes by designation, tone, intent, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        \"Optional metadata pivots (Intent/Designation/Tone) are only available when those fields exist—skipping for this dataset.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "source": [
    "### 10. Axial coding snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b79bc585a40fcaf58bf750017e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_modes_df = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT fm.display_name, count(*) AS occurrences\n",
    "    FROM axial_links al\n",
    "    JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
    "    WHERE al.run_id = ?\n",
    "    GROUP BY 1\n",
    "    ORDER BY occurrences DESC\n",
    "    \"\"\",\n",
    "    (ACTIVE_RUN_ID,),\n",
    ").df()\n",
    "\n",
    "if failure_modes_df.empty:\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"⚠️ No failure modes linked yet—use the web tool (F) to start axial coding.\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    display(failure_modes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916684f9a58a4a2aa5f864670399430d",
   "metadata": {},
   "source": [
    "### 11. Failure-mode × Intent co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671c31a24314836a5b85d7ef7fbf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failure_modes_df.empty and not annotations_df.empty:\n",
    "    axial_df = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT al.annotation_id, al.failure_mode_id, fm.display_name, a.email_hash\n",
    "        FROM axial_links al\n",
    "        JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
    "        JOIN annotations a ON al.annotation_id = a.annotation_id\n",
    "        WHERE al.run_id = ?\n",
    "        \"\"\",\n",
    "        (ACTIVE_RUN_ID,),\n",
    "    ).df()\n",
    "\n",
    "    if not axial_df.empty:\n",
    "        merged = axial_df.merge(\n",
    "            annotations_df[[\"annotation_id\", \"Intent\"]], on=\"annotation_id\", how=\"left\"\n",
    "        )\n",
    "        co_matrix = pd.crosstab(merged[\"display_name\"], merged[\"Intent\"])\n",
    "        display(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0902fd34d4ace834912fa1002cf8e",
   "metadata": {},
   "source": [
    "### 12. Export for sharing\n",
    "Write flattened tables so facilitators/stakeholders can review outside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa52606d8c4a75a9b52967216f8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT = False  # toggle to True to write CSV/JSON snapshots\n",
    "\n",
    "if EXPORT:\n",
    "    export_annotations = DATA_DIR / f\"email_annotations_{ACTIVE_RUN_ID}.csv\"\n",
    "    export_failure_modes = DATA_DIR / f\"failure_modes_{ACTIVE_RUN_ID}.csv\"\n",
    "    annotations_df.to_csv(export_annotations, index=False)\n",
    "    failure_modes_df.to_csv(export_failure_modes, index=False)\n",
    "    Markdown(\n",
    "        f\"Exported annotations → `{export_annotations.name}` and failure modes → `{export_failure_modes.name}`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1fa73e5044315a093ec459c9be902",
   "metadata": {},
   "source": [
    "---\n",
    "**Next steps:**\n",
    "1. Ensure each major failure mode has ≥20 failing examples before moving to Notebook 03.\n",
    "2. Re-run the prompt (Notebook 01) + trace generator after prompt fixes so you can compare runs (`run_id` == short Git SHA).\n",
    "3. Log qualitative takeaways and TODOs in the facilitation tracker / plan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evals-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}