{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 02: Open & Axial Coding Walkthrough\n",
        "\n",
        "Grounded-theory error analysis for the email summariser workshop. Use this notebook to link qualitative insights from the annotation tool back into the Analyze \u2192 Measure \u2192 Improve loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prerequisites\n",
        "- Run Notebook 00/01a to prepare either the filtered or synthetic email CSV.\n",
        "- Use `tools/generate_email_traces.py` to create traces for the current Git commit (short SHA stored as the `run_id`). The optional cell below lets you run it in-place during a workshop demo.\n",
        "- Launch `tools/email_annotation_app.py` to collect open coding notes and failure modes in the browser, then return here to analyse the DuckDB tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "from IPython.display import display, Markdown\n",
        "import ipywidgets as widgets\n",
        "\n",
        "DATA_DIR = Path('../data')\n",
        "TRACE_ROOT = Path('../annotation/traces')\n",
        "DUCKDB_PATH = DATA_DIR / 'email_annotations.duckdb'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Choose data source\n",
        "Pick between the filtered production slice and the synthetic seed set from Notebook 01a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_OPTIONS = {\n",
        "    'filtered': DATA_DIR / 'filtered_emails.csv',\n",
        "    'synthetic': DATA_DIR / 'synthetic_emails.csv',\n",
        "}\n",
        "\n",
        "source_toggle = widgets.ToggleButtons(\n",
        "    options=[('Filtered Emails', 'filtered'), ('Synthetic Seed Set', 'synthetic')],\n",
        "    value='filtered' if (DATA_DIR / 'filtered_emails.csv').exists() else 'synthetic',\n",
        "    description='Dataset:',\n",
        "    style={'description_width': 'initial'},\n",
        ")\n",
        "\n",
        "display(source_toggle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SELECTED_KEY = source_toggle.value\n",
        "DATA_SOURCE_PATH = DATA_OPTIONS[SELECTED_KEY]\n",
        "if not DATA_SOURCE_PATH.exists():\n",
        "    raise FileNotFoundError(f'Expected dataset at {DATA_SOURCE_PATH}. Run Notebook 00 or 01a first.')\n",
        "\n",
        "Markdown(f'**Using:** `{DATA_SOURCE_PATH}`')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. (Optional) Regenerate traces for this run\n",
        "Set the flag to `True` to call `tools/generate_email_traces.py` using the selected dataset. The script writes trace JSON under `annotation/traces/<run_id>` and upserts rows into DuckDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_TRACE_GENERATOR = False  # toggle to True for live demos\n",
        "\n",
        "if RUN_TRACE_GENERATOR:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        str(Path('../tools/generate_email_traces.py')),\n",
        "        '--emails',\n",
        "        str(DATA_SOURCE_PATH.resolve()),\n",
        "        '--out',\n",
        "        str(TRACE_ROOT.resolve()),\n",
        "    ]\n",
        "    print('Running:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Verify available trace runs\n",
        "Each trace directory name equals the short Git SHA captured when the generator script ran."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "available_runs = []\n",
        "if TRACE_ROOT.exists():\n",
        "    available_runs = sorted(p.name for p in TRACE_ROOT.iterdir() if p.is_dir())\n",
        "\n",
        "if not available_runs:\n",
        "    raise RuntimeError('No trace runs detected under ../annotation/traces. Generate traces before proceeding.')\n",
        "\n",
        "ACTIVE_RUN_ID = available_runs[-1]\n",
        "Markdown(f'**Active run:** `{ACTIVE_RUN_ID}` (showing most recent)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Connect to DuckDB and inspect trace runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conn = duckdb.connect(str(DUCKDB_PATH))\n",
        "\n",
        "runs_df = conn.execute(\n",
        "    'SELECT run_id, prompt_path, source_csv, generated_at FROM trace_runs ORDER BY generated_at DESC'\n",
        ").df()\n",
        "\n",
        "display(runs_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Preview the chosen dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emails_df = pd.read_csv(DATA_SOURCE_PATH)\n",
        "Markdown(f'Loaded **{len(emails_df):,}** emails from `{DATA_SOURCE_PATH.name}`')\n",
        "emails_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Emails registered for the active run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emails_raw_df = conn.execute(\n",
        "    '''\n",
        "    SELECT email_id, subject, metadata, run_id, ingested_at\n",
        "    FROM emails_raw\n",
        "    WHERE run_id = ?\n",
        "    ORDER BY email_id\n",
        "    '''\n",
        "    , (ACTIVE_RUN_ID,)\n",
        ").df()\n",
        "\n",
        "if emails_raw_df.empty:\n",
        "    raise RuntimeError(f'No emails found in DuckDB for run_id={ACTIVE_RUN_ID}. Re-run the trace generator.')\n",
        "\n",
        "emails_raw_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Open coding inventory\n",
        "Pull annotations captured through the web tool (auto-saved to DuckDB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "annotations_df = conn.execute(\n",
        "    '''\n",
        "    SELECT email_id, annotation_id, labeler_id, open_code, pass_fail, created_at\n",
        "    FROM annotations\n",
        "    WHERE run_id = ?\n",
        "    ORDER BY created_at\n",
        "    '''\n",
        "    , (ACTIVE_RUN_ID,)\n",
        ").df()\n",
        "\n",
        "Markdown(f'Collected **{len(annotations_df):,}** annotations for run `{ACTIVE_RUN_ID}`')\n",
        "annotations_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Pass/Fail mix and annotation velocity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if annotations_df.empty:\n",
        "    display(Markdown('\u26a0\ufe0f No annotations logged yet\u2014open the web tool and capture a few observations.'))\n",
        "else:\n",
        "    annotations_df['status'] = annotations_df['pass_fail'].map({True: 'Fail', False: 'Pass', None: 'Unknown'})\n",
        "    summary = annotations_df.groupby('status').size().reset_index(name='count')\n",
        "    display(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Join annotations with email metadata\n",
        "Use JSON metadata stored in `emails_raw` to segment open codes by designation, tone, intent, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not annotations_df.empty:\n",
        "    metadata_records = emails_raw_df.set_index('email_id')['metadata'].to_dict()\n",
        "    def extract_field(email_id: str, field: str):\n",
        "        raw = metadata_records.get(email_id)\n",
        "        if not raw:\n",
        "            return None\n",
        "        try:\n",
        "            data = json.loads(raw)\n",
        "        except json.JSONDecodeError:\n",
        "            return None\n",
        "        value = data.get(field)\n",
        "        if isinstance(value, (list, tuple)):\n",
        "            return ', '.join(str(v) for v in value)\n",
        "        return value\n",
        "\n",
        "    annotations_df['Intent'] = annotations_df['email_id'].apply(lambda eid: extract_field(eid, 'Intent'))\n",
        "    annotations_df['Designation'] = annotations_df['email_id'].apply(lambda eid: extract_field(eid, 'Designation'))\n",
        "    pivot = pd.crosstab(annotations_df['Intent'], annotations_df['status'])\n",
        "    display(pivot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Axial coding snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "failure_modes_df = conn.execute(\n",
        "    '''\n",
        "    SELECT fm.display_name, count(*) AS occurrences\n",
        "    FROM axial_links al\n",
        "    JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
        "    WHERE al.run_id = ?\n",
        "    GROUP BY 1\n",
        "    ORDER BY occurrences DESC\n",
        "    '''\n",
        "    , (ACTIVE_RUN_ID,)\n",
        ").df()\n",
        "\n",
        "if failure_modes_df.empty:\n",
        "    display(Markdown('\u26a0\ufe0f No failure modes linked yet\u2014use the web tool (F) to start axial coding.'))\n",
        "else:\n",
        "    display(failure_modes_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11. Failure-mode \u00d7 Intent co-occurrence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not failure_modes_df.empty and not annotations_df.empty:\n",
        "    axial_df = conn.execute(\n",
        "        '''\n",
        "        SELECT al.annotation_id, al.failure_mode_id, fm.display_name, a.email_id\n",
        "        FROM axial_links al\n",
        "        JOIN failure_modes fm ON al.failure_mode_id = fm.failure_mode_id\n",
        "        JOIN annotations a ON al.annotation_id = a.annotation_id\n",
        "        WHERE al.run_id = ?\n",
        "        '''\n",
        "        , (ACTIVE_RUN_ID,)\n",
        "    ).df()\n",
        "\n",
        "    if not axial_df.empty:\n",
        "        merged = axial_df.merge(annotations_df[['annotation_id', 'Intent']], on='annotation_id', how='left')\n",
        "        co_matrix = pd.crosstab(merged['display_name'], merged['Intent'])\n",
        "        display(co_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12. Export for sharing\n",
        "Write flattened tables so facilitators/stakeholders can review outside the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPORT = False  # toggle to True to write CSV/JSON snapshots\n",
        "\n",
        "if EXPORT:\n",
        "    export_annotations = DATA_DIR / f'email_annotations_{ACTIVE_RUN_ID}.csv'\n",
        "    export_failure_modes = DATA_DIR / f'failure_modes_{ACTIVE_RUN_ID}.csv'\n",
        "    annotations_df.to_csv(export_annotations, index=False)\n",
        "    failure_modes_df.to_csv(export_failure_modes, index=False)\n",
        "    Markdown(f'Exported annotations \u2192 `{export_annotations.name}` and failure modes \u2192 `{export_failure_modes.name}`')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Next steps:**\n",
        "1. Ensure each major failure mode has \u226520 failing examples before moving to Notebook 03.\n",
        "2. Re-run the prompt (Notebook 01) + trace generator after prompt fixes so you can compare runs (`run_id` == short Git SHA).\n",
        "3. Log qualitative takeaways and TODOs in the facilitation tracker / plan."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}