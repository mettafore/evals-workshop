{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c842dcf",
   "metadata": {},
   "source": [
    "# Explore Email Dataset\n",
    "\n",
    "We start with the full Enron email dump (`emails.csv`, ~517k raw messages) and build a\n",
    "small, workshop-ready subset of emails that already contain rich conversational\n",
    "context in their quoted history. The goal is to hand facilitators a handful of\n",
    "messages that:\n",
    "\n",
    "- revolve around action-oriented language (deadlines, follow ups, approvals)\n",
    "- keep recipient lists manageable (true conversations, not company-wide blasts)\n",
    "- include enough quoted history to surface commitments and potential misses\n",
    "- stay short enough (≤5k characters) for a 60-minute tutorial\n",
    "\n",
    "Rather than reconstructing threads, we treat each long email as the unit of\n",
    "analysis. By the end, we will have a scored table (`candidate_df`) packed with the\n",
    "most useful conversation-heavy emails, ready for manual evaluation and LLM judge\n",
    "demos.\n",
    "\n",
    "\n",
    "We cap subject frequency at `MAX_SUBJECT_FREQUENCY = 50` so recurring broadcast topics (e.g., newsletters) don’t dominate the slice.\n",
    "\n",
    "We apply a deterministic filter stack: `TIME_WINDOW`, `ACTION_KEYWORDS`, `MAX_RECIPIENTS`, `BROADCAST_SUBJECT_KEYWORDS`, `MAX_SUBJECT_FREQUENCY`, `MIN_QUOTE_MARKERS`, and `MAX_BODY_CHARS` to end up with email-sized conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc9fcb",
   "metadata": {},
   "source": [
    "### What We'll Do\n",
    "- Load the raw Enron CSV and take a quick peek at the MIME payloads.\n",
    "- Parse headers/body into lightweight features (participants, body length, quote markers).\n",
    "- Apply a focused filter stack: time window, action keywords, recipient cap, broadcast stoplist, long/quoted requirement, a subject frequency cap, and a 5k character ceiling.\n",
    "- Inspect distributions and rank the remaining emails by conversational depth.\n",
    "- Preview the top candidates so facilitators can export them for labeling and evals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from email.utils import getaddresses, parseaddr\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "DATA_PATH = Path(\"../data/emails.csv\")\n",
    "TIME_WINDOW = (pd.Timestamp(\"2001-03-01\"), pd.Timestamp(\"2001-06-30\"))\n",
    "ACTION_KEYWORDS = [\n",
    "    \"deadline\",\n",
    "    \"deliver\",\n",
    "    \"deliverable\",\n",
    "    \"please review\",\n",
    "    \"fyi\",\n",
    "    \"action item\",\n",
    "    \"follow up\",\n",
    "    \"schedule\",\n",
    "    \"update\",\n",
    "    \"approve\",\n",
    "]\n",
    "ACTION_PATTERN = re.compile(\n",
    "    \"|\".join(re.escape(k) for k in ACTION_KEYWORDS), flags=re.IGNORECASE\n",
    ")\n",
    "BROADCAST_SUBJECT_KEYWORDS = [\n",
    "    \"newsletter\",\n",
    "    \"announcement\",\n",
    "    \"enron mentions\",\n",
    "    \"organizational announcement\",\n",
    "]\n",
    "MAX_RECIPIENTS = 6\n",
    "MIN_BODY_CHARS = 800\n",
    "MIN_QUOTE_MARKERS = 2\n",
    "MAX_BODY_CHARS = 5000\n",
    "MAX_SUBJECT_FREQUENCY = 50\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34efda",
   "metadata": {},
   "source": [
    "## Load raw emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c37a9",
   "metadata": {},
   "source": [
    "We ingest the full `emails.csv`, compute quick size stats, and immediately apply a coarse action-keyword + length filter so we only carry interesting messages into later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(DATA_PATH, usecols=[\"file\", \"message\"]).rename(\n",
    "    columns={\"message\": \"raw_message\"}\n",
    ")\n",
    "raw_df[\"raw_char_len\"] = raw_df[\"raw_message\"].str.len()\n",
    "print(\n",
    "    f\"Loaded {len(raw_df):,} rows (~{raw_df.raw_char_len.sum() / 1e6:,.1f}M characters)\"\n",
    ")\n",
    "\n",
    "keyword_regex = \"|\".join(re.escape(k) for k in ACTION_KEYWORDS)\n",
    "raw_df = raw_df[\n",
    "    raw_df[\"raw_message\"].str.contains(keyword_regex, case=False, regex=True, na=False)\n",
    "]\n",
    "raw_df = raw_df[raw_df[\"raw_char_len\"] >= MIN_BODY_CHARS]\n",
    "print(f\"After quick keyword/length filter: {len(raw_df):,} rows remain\")\n",
    "\n",
    "display(raw_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1646e9",
   "metadata": {},
   "source": [
    "### Peek at a raw message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060a592",
   "metadata": {},
   "source": [
    "Before parsing, glance at one raw MIME payload so you remember what the quotes and headers look like in the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.sample(1)[\"raw_message\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b80da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render a sampled raw message as a layperson-friendly preview\n",
    "sample_row = raw_df.sample(1).iloc[0]\n",
    "raw_message = sample_row[\"raw_message\"]\n",
    "\n",
    "header_lines: list[str] = []\n",
    "body_lines: list[str] = []\n",
    "found_break = False\n",
    "for line in raw_message.splitlines():\n",
    "    if not found_break and line.strip() == \"\":\n",
    "        found_break = True\n",
    "        continue\n",
    "    if found_break:\n",
    "        body_lines.append(line.rstrip())\n",
    "    else:\n",
    "        header_lines.append(line.rstrip())\n",
    "\n",
    "header_preview = \"\\n\".join(header_lines)\n",
    "body_preview = \"\\n\".join(body_lines[:40]).strip()\n",
    "if len(body_lines) > 40:\n",
    "    body_preview += \"\\n...\"\n",
    "body_preview = body_preview or \"[empty body]\"\n",
    "\n",
    "email_title = sample_row[\"file\"]\n",
    "\n",
    "markdown = (\n",
    "    f\"### Sample email: {email_title}\\n\\n\"\n",
    "    \"**Header preview**\\n\\n\"\n",
    "    \"```\\n\"\n",
    "    f\"{header_preview}\\n\"\n",
    "    \"```\\n\\n\"\n",
    "    \"**Body preview**\\n\\n\"\n",
    "    \"```\\n\"\n",
    "    f\"{body_preview}\\n\"\n",
    "    \"```\"\n",
    ")\n",
    "\n",
    "display(Markdown(markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a849b",
   "metadata": {},
   "source": [
    "## Parse headers and derive features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9861b",
   "metadata": {},
   "source": [
    "Split headers/body, normalize the subject, and extract lightweight features (participants, body length, quote markers) that we'll use for filtering and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_BREAK = re.compile(r\"\\r?\\n\\r?\\n\")\n",
    "SUBJECT_PREFIX_RE = re.compile(r\"^(re|fw|fwd):\\s*\", flags=re.IGNORECASE)\n",
    "SEPARATOR_RE = re.compile(r\"-{3,}\\s*Original Message\", flags=re.IGNORECASE)\n",
    "QUOTE_LINE_RE = re.compile(r\"^(>+)\", flags=re.MULTILINE)\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def split_headers_body(raw: str) -> tuple[str, str]:\n",
    "    if not isinstance(raw, str):\n",
    "        return \"\", \"\"\n",
    "    parts = HEADER_BREAK.split(raw, maxsplit=1)\n",
    "    header_block = parts[0] if parts else \"\"\n",
    "    body = parts[1] if len(parts) > 1 else \"\"\n",
    "    return header_block, body\n",
    "\n",
    "\n",
    "def parse_header_block(block: str) -> dict[str, str]:\n",
    "    headers: dict[str, str] = {}\n",
    "    current_key: str | None = None\n",
    "    for line in block.splitlines():\n",
    "        if not line:\n",
    "            current_key = None\n",
    "            continue\n",
    "        if line.startswith((\" \", \"\\t\")) and current_key:\n",
    "            headers[current_key] += f\" {line.strip()}\"\n",
    "            continue\n",
    "        if \":\" not in line:\n",
    "            current_key = None\n",
    "            continue\n",
    "        key, value = line.split(\":\", 1)\n",
    "        current_key = key.strip().lower()\n",
    "        headers[current_key] = value.strip()\n",
    "    return headers\n",
    "\n",
    "\n",
    "def normalize_subject(subject: str) -> str:\n",
    "    if not subject:\n",
    "        return \"\"\n",
    "    cleaned = subject\n",
    "    for _ in range(5):\n",
    "        updated = SUBJECT_PREFIX_RE.sub(\"\", cleaned)\n",
    "        if updated == cleaned:\n",
    "            break\n",
    "        cleaned = updated\n",
    "    cleaned = WHITESPACE_RE.sub(\" \", cleaned)\n",
    "    return cleaned.strip().lower()\n",
    "\n",
    "\n",
    "def parse_email_fields(raw: str) -> dict[str, object]:\n",
    "    header_block, body = split_headers_body(raw)\n",
    "    headers = parse_header_block(header_block)\n",
    "\n",
    "    subject = headers.get(\"subject\", \"\").strip()\n",
    "    normalized_subject = normalize_subject(subject)\n",
    "\n",
    "    date_raw = headers.get(\"date\", \"\").strip() or None\n",
    "    from_raw = headers.get(\"from\", \"\").strip() or None\n",
    "    from_email = parseaddr(from_raw)[1].lower() if from_raw else None\n",
    "\n",
    "    to_raw = headers.get(\"to\", \"\").strip() or None\n",
    "    to_emails = [addr.lower() for _, addr in getaddresses([to_raw])] if to_raw else []\n",
    "    cc_raw = headers.get(\"cc\", \"\").strip() or None\n",
    "    cc_emails = [addr.lower() for _, addr in getaddresses([cc_raw])] if cc_raw else []\n",
    "\n",
    "    body_clean = body.strip()\n",
    "    quote_separator_count = len(SEPARATOR_RE.findall(body))\n",
    "    quote_line_count = len(QUOTE_LINE_RE.findall(body))\n",
    "\n",
    "    return {\n",
    "        \"subject\": subject or None,\n",
    "        \"normalized_subject\": normalized_subject or None,\n",
    "        \"from_raw\": from_raw,\n",
    "        \"from_email\": from_email,\n",
    "        \"to_raw\": to_raw,\n",
    "        \"to_emails\": \";\".join(to_emails) or None,\n",
    "        \"to_count\": len(to_emails),\n",
    "        \"cc_raw\": cc_raw,\n",
    "        \"cc_emails\": \";\".join(cc_emails) or None,\n",
    "        \"cc_count\": len(cc_emails),\n",
    "        \"date_raw\": date_raw,\n",
    "        \"body\": body_clean,\n",
    "        \"body_char_len\": len(body_clean),\n",
    "        \"body_line_count\": body_clean.count(\"\\n\") + 1 if body_clean else 0,\n",
    "        \"quote_separator_count\": quote_separator_count,\n",
    "        \"quote_line_count\": quote_line_count,\n",
    "        \"action_hit\": bool(ACTION_PATTERN.search(f\"{subject}\\n{body}\"))\n",
    "        if subject or body\n",
    "        else False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68420033",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_records = raw_df[\"raw_message\"].apply(parse_email_fields).apply(pd.Series)\n",
    "parsed_df = pd.concat([raw_df.drop(columns=[\"raw_message\"]), parsed_records], axis=1)\n",
    "parsed_df[\"sent_at\"] = pd.to_datetime(\n",
    "    parsed_df[\"date_raw\"], errors=\"coerce\", utc=True\n",
    ").dt.tz_localize(None)\n",
    "parsed_df = parsed_df.dropna(subset=[\"body\"])\n",
    "parsed_df = parsed_df[parsed_df[\"body_char_len\"] > 0]\n",
    "\n",
    "print(f\"Parsed rows: {len(parsed_df):,}\")\n",
    "display(\n",
    "    parsed_df[\n",
    "        [\n",
    "            \"file\",\n",
    "            \"sent_at\",\n",
    "            \"from_email\",\n",
    "            \"subject\",\n",
    "            \"body_char_len\",\n",
    "            \"quote_separator_count\",\n",
    "        ]\n",
    "    ].head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2145ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df[\"quote_separator_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching the parsed DataFrame for future use\n",
    "parsed_df.to_csv(\"../data/parsed_emails.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = pd.read_csv(\"../data/parsed_emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e188c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cda5e0",
   "metadata": {},
   "source": [
    "## Filter for workshop-ready emails\n",
    "\n",
    "Limit the dataset to action-heavy, conversational emails using a series of transparent filters: time window, keyword confirmation, recipient cap, broadcast removal, subject frequency cut, long/quoted requirement, and a 5k ceiling.\n",
    "\n",
    "Filters applied in order:\n",
    "1. Time window (`TIME_WINDOW`)\n",
    "2. Keyword confirmation (`ACTION_KEYWORDS`)\n",
    "3. Recipient cap (`MAX_RECIPIENTS`)\n",
    "4. Broadcast subject removal (`BROADCAST_SUBJECT_KEYWORDS`)\n",
    "5. Subject frequency cap (`MAX_SUBJECT_FREQUENCY`)\n",
    "6. Conversation depth (`MIN_QUOTE_MARKERS` or extended length)\n",
    "7. Body length cap (`MAX_BODY_CHARS`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snapshot(df: pd.DataFrame, stage: str) -> dict[str, object]:\n",
    "    return {\n",
    "        \"stage\": stage,\n",
    "        \"emails\": len(df),\n",
    "        \"median_body_chars\": int(df[\"body_char_len\"].median()) if not df.empty else 0,\n",
    "        \"median_quote_separators\": int(df[\"quote_separator_count\"].median())\n",
    "        if not df.empty\n",
    "        else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "filtered_df = parsed_df.copy()\n",
    "filtered_df[\"sent_at\"] = pd.to_datetime(filtered_df[\"sent_at\"], errors=\"coerce\")\n",
    "progress = [snapshot(filtered_df, \"parsed\")]\n",
    "\n",
    "if TIME_WINDOW:\n",
    "    start, end = TIME_WINDOW\n",
    "    time_mask = filtered_df[\"sent_at\"].between(start, end, inclusive=\"both\")\n",
    "    filtered_df = filtered_df[time_mask]\n",
    "    progress.append(snapshot(filtered_df, f\"time window {start.date()}→{end.date()}\"))\n",
    "\n",
    "filtered_df = filtered_df[filtered_df[\"action_hit\"]]\n",
    "progress.append(snapshot(filtered_df, \"keyword confirmed post-parse\"))\n",
    "\n",
    "recipient_mask = (\n",
    "    filtered_df[\"to_count\"].fillna(0) + filtered_df[\"cc_count\"].fillna(0)\n",
    ") <= MAX_RECIPIENTS\n",
    "filtered_df = filtered_df[recipient_mask]\n",
    "progress.append(snapshot(filtered_df, f\"recipient count ≤ {MAX_RECIPIENTS}\"))\n",
    "\n",
    "\n",
    "broadcast_mask = ~filtered_df[\"normalized_subject\"].fillna(\"\").str.contains(\n",
    "    \"|\".join(re.escape(k) for k in BROADCAST_SUBJECT_KEYWORDS), case=False, regex=True\n",
    ")\n",
    "filtered_df = filtered_df[broadcast_mask]\n",
    "progress.append(snapshot(filtered_df, \"broadcast subjects dropped\"))\n",
    "\n",
    "subject_counts = filtered_df[\"normalized_subject\"].fillna(\"(no subject)\").value_counts()\n",
    "freq_mask = (\n",
    "    filtered_df[\"normalized_subject\"].fillna(\"(no subject)\").map(subject_counts)\n",
    "    <= MAX_SUBJECT_FREQUENCY\n",
    ")\n",
    "filtered_df = filtered_df[freq_mask]\n",
    "progress.append(snapshot(filtered_df, f\"subject frequency ≤ {MAX_SUBJECT_FREQUENCY}\"))\n",
    "\n",
    "conversation_mask = (\n",
    "    filtered_df[\"quote_separator_count\"] + filtered_df[\"quote_line_count\"]\n",
    "    >= MIN_QUOTE_MARKERS\n",
    ") | (filtered_df[\"body_char_len\"] >= MIN_BODY_CHARS * 1.5)\n",
    "filtered_df = filtered_df[conversation_mask]\n",
    "progress.append(snapshot(filtered_df, \"long/quoted conversations\"))\n",
    "\n",
    "filtered_df = filtered_df[filtered_df[\"body_char_len\"] <= MAX_BODY_CHARS]\n",
    "progress.append(snapshot(filtered_df, f\"body length ≤ {MAX_BODY_CHARS:,}\"))\n",
    "\n",
    "progress_df = pd.DataFrame(progress)\n",
    "display(progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c5140",
   "metadata": {},
   "source": [
    "## Explore the filtered distribution\n",
    "\n",
    "Check how body lengths and quote markers behave after the filters.\n",
    "\n",
    "Look for a right-skewed distribution (a few long emails) but avoid extreme tails that would overwhelm participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefcf4f",
   "metadata": {},
   "source": [
    "Check how body lengths and quote markers behave after the filters so we know the remaining sample is both rich in context and manageable for participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82995cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df = filtered_df.copy()\n",
    "scored_df[\"quote_markers\"] = (\n",
    "    scored_df[\"quote_separator_count\"] + scored_df[\"quote_line_count\"]\n",
    ")\n",
    "scored_df = scored_df.sort_values(\n",
    "    [\"quote_markers\", \"body_char_len\"], ascending=[False, False]\n",
    ")\n",
    "preview_cols = [\n",
    "    \"file\",\n",
    "    \"subject\",\n",
    "    \"from_email\",\n",
    "    \"to_emails\",\n",
    "    \"body_char_len\",\n",
    "    \"quote_markers\",\n",
    "]\n",
    "display(scored_df[preview_cols].head(10))\n",
    "print(f\"Final candidate emails: {len(scored_df):,}\")\n",
    "\n",
    "candidate_df = scored_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed620bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    filtered_df[\"body_char_len\"].plot(kind=\"hist\", bins=40, ax=axes[0], color=\"#1f77b4\")\n",
    "    axes[0].set_title(\"Body length distribution\")\n",
    "    axes[0].set_xlabel(\"Characters\")\n",
    "\n",
    "    (filtered_df[\"quote_separator_count\"] + filtered_df[\"quote_line_count\"]).plot(\n",
    "        kind=\"hist\",\n",
    "        bins=range(0, filtered_df[\"quote_separator_count\"].max() + 3),\n",
    "        ax=axes[1],\n",
    "        color=\"#ff7f0e\",\n",
    "    )\n",
    "    axes[1].set_title(\"Quote marker distribution\")\n",
    "    axes[1].set_xlabel(\"Markers per email\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No emails matched the current filter settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d182f",
   "metadata": {},
   "source": [
    "## Review top candidate emails\n",
    "\n",
    "Inspect a few high-scoring messages to confirm the slice feels right for the workshop.\n",
    "\n",
    "Use these previews to pick golden examples; export them straight into the labeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7963873",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a70f5",
   "metadata": {},
   "source": [
    "Rank the filtered emails by conversational depth, then preview a few to confirm they are suitable for the workshop exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0206ba",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Export `candidate_df` for annotation or golden-set authoring.\n",
    "- Drop selected emails into the evaluation notebooks to practice the Analyze→Measure→Improve loop.\n",
    "- Adjust the thresholds (`ACTION_KEYWORDS`, `MAX_RECIPIENTS`, `MIN_QUOTE_MARKERS`, `MAX_BODY_CHARS`) if the balance between context and readability needs tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_email(row: pd.Series, body_lines: int = 40) -> None:\n",
    "    subject = row[\"subject\"] or \"(no subject)\"\n",
    "    meta_lines = [\n",
    "        f\"### {subject}\",\n",
    "        f\"- File: `{row['file']}`\",\n",
    "        f\"- From: {row.get('from_email') or row.get('from_raw') or 'unknown'}\",\n",
    "        f\"- To: {row.get('to_emails') or row.get('to_raw') or '—'}\",\n",
    "        f\"- CC: {row.get('cc_emails') or row.get('cc_raw') or '—'}\",\n",
    "        f\"- Sent: {row['sent_at']:%Y-%m-%d %H:%M}\"\n",
    "        if pd.notna(row[\"sent_at\"])\n",
    "        else \"- Sent: unknown\",\n",
    "        f\"- Characters: {row['body_char_len']:,}\",\n",
    "        f\"- Quote markers: {row['quote_separator_count'] + row['quote_line_count']:,}\",\n",
    "    ]\n",
    "    body_lines_list = row[\"body\"].splitlines()\n",
    "    body_preview = \"\\n\".join(body_lines_list[:body_lines]).strip()\n",
    "    if len(body_lines_list) > body_lines:\n",
    "        body_preview += \"\\n...\"\n",
    "    markdown = \"\\n\\n\".join(meta_lines) + f\"\\n\\n```\\n{body_preview}\\n```\"\n",
    "    display(Markdown(markdown))\n",
    "\n",
    "\n",
    "if not filtered_df.empty:\n",
    "    top_rows = (\n",
    "        candidate_df.assign(\n",
    "            quote_markers=lambda df: df[\"quote_separator_count\"]\n",
    "            + df[\"quote_line_count\"]\n",
    "        )\n",
    "        .sort_values([\"quote_markers\", \"body_char_len\"], ascending=[False, False])\n",
    "        .head(100)[97:98]  # Show the 101st email for variety\n",
    "    )\n",
    "    for _, row in top_rows.iterrows():\n",
    "        render_email(row, 1000)\n",
    "else:\n",
    "    print(\"No emails available to preview.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05670af",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df[\"body_char_len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e222d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df.to_csv(\"../data/filtered_emails.csv\", index=False)\n",
    "print(\"Filtered emails saved to filtered_emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50969a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df = pd.read_csv(\"../data/filtered_emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df.shape\n",
    "candidate_df.sample(30).to_csv(\"../data/sample_filtered_emails_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df054a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "filtered_path = Path(\"../data/filtered_emails.csv\")\n",
    "if filtered_path.exists():\n",
    "    df = pd.read_csv(filtered_path)\n",
    "\n",
    "    def compute_email_hash(row):\n",
    "        joined = \"||\".join(\n",
    "            str(row.get(col, \"\")) for col in df.columns if col != \"email_hash\"\n",
    "        )\n",
    "        return hashlib.sha256(joined.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    df[\"email_hash\"] = df.apply(compute_email_hash, axis=1)\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=\"email_hash\")\n",
    "    removed = before - len(df)\n",
    "    if removed:\n",
    "        print(f\"Removed {removed} duplicate emails detected via email_hash\")\n",
    "    df.to_csv(filtered_path, index=False)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"filtered_emails.csv not found; run earlier cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.sample(30).to_csv(\"../data/filtered_emails_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50745c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c09931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evals-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
